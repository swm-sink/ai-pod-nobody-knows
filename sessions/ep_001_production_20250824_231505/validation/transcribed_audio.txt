What if I told you the world's smartest AI researchers just admitted they have no idea what they're doing? I mean it. The actual experts, Nobel laureates, Turing Award winners, the people building the technology that's about to reshape everything, they just dropped a 298-page report and it basically says, "Yeah, we're winging it too." But before you panic, this might be the most reassuring thing you hear all day. Welcome to Nobody Knows?, the podcast that celebrates what we know and what we don't know. I'm your host, and today, we're diving into something fascinating. We're exploring why the smartest people in AI freely admit their confusion, why that makes them more trustworthy, not less, and how understanding this can completely change the way you think about expertise itself. Because here's the thing. If you're confused about AI, you're in excellent company. On January 29th, 2025, something remarkable happened. The world's leading AI experts published what might be the most honest document ever written about artificial intelligence. It's called the 2025 International AI Safety Report, 298 pages, backed by 30 nations, the OECD, the UN, and the EU, written by over a hundred independent experts, including Nobel laureates and Turing Award winners. And right there on the very first page, they made an admission that should have made headlines everywhere: The future of general-purpose AI is remarkably uncertain. Now, this isn't some fringe group of skeptics. This report was led by literally one of the godfathers of modern AI. This is the man who won the Turing Award for deep learning breakthroughs. He's not some outsider throwing stones. He's the architect of the building. But here's what's fascinating about this moment. When I first read that line, "remarkably uncertain", my initial reaction was disappointment. Wait, these are the experts. Aren't they supposed to have answers? And then I caught myself. Why was I disappointed by honesty? Why did I want them to lie to me with false confidence? Because that's what we've been trained to expect from experts, isn't it? We want them to stand at podiums and speak with absolute certainty. We want them to have all the answers even when the questions are impossibly complex. But what if that's exactly backwards? What if the smartest thing these experts could do is exactly what they did: admit what they don't know? Here's where it gets really interesting. Let me show you something that will probably surprise you. The Pew Research Center published a study in April 2025. They looked at AI perceptions. They surveyed both AI experts and the general public about their expectations for AI's impact. Here's what they found. 56% of AI experts expect AI to have a positive impact on society, but only 17% of the general public feels the same way. Read that again. The experts, the people who actually understand this technology, are nearly three times more optimistic than the rest of us. There's a 39 percentage point gap between expert optimism and public pessimism, and it runs completely opposite to what we might expect. Think about that for a moment. We assume experts are more worried because they see all the risks we don't, but actually, they're more hopeful because they understand the possibilities we can't see. Meanwhile, we're out here convinced that the people building AI are recklessly optimistic techno-utopians who don't see the dangers, but the data shows the complete opposite. They're the cautious ones, and they're still more optimistic than we are. So why does their admission of uncertainty feel threatening instead of reassuring? Let me tell you about Geoffrey Hinton. You know him as the godfather of AI, another Turing Award winner, the guy who basically invented the neural network that power ChatGPT, and everything else we're talking about today. In July 2025, Hinton did something remarkable in a podcast interview. He said, and I quote, "There could come a point when humans can't understand what AI is thinking or planning to do." Then he went further, "I wouldn't be surprised if they developed their own language for thinking, and we have no idea what they're thinking." Now, this is the man who created the foundation for all of this. If anyone should understand AI thinking, it's Geoffrey Hinton. But instead of pretending he has all the answers, he's saying, "I built this, and even I don't fully understand where it's going." Is that terrifying, or is it exactly what you'd want to hear from someone with that much responsibility? Now, your brain might be protesting this idea. Here's where the psychology gets really fascinating. There's a whole body of research on something called intellectual humility, the willingness to admit what you don't know. And it turns out when experts acknowledge uncertainty, something counterintuitive happens. They become more trustworthy, not less.Your brain is wired to detect false confidence. When someone claims to know more than they actually do, red flags start going up. But when someone demonstrates intellectual humility, when they clearly distinguish between what they know and what they don't, your trust actually increases. Studies from UC Irvine published in Nature Machine Intelligence found something fascinating. There's a huge gap between what AI systems actually know and what people think they know. We systematically overestimate AI capabilities while underestimating their limitations. But here's the kicker. When AI experts explain both the capabilities and the limitations clearly, people become more comfortable with the technology, not less. It's like your brain has a built-in bullshit detector. That detector relaxes when someone says, "I don't know," but it gets suspicious when someone claims to know everything. Think about your own experience. Who do you trust more? The person who admits they might be wrong or the person who never shows any doubt? The research is clear. Intellectual humility signals competence in complex situations. False confidence signals either ignorance or deception. So when Hinton says, "I don't fully understand what I've created," he's not revealing weakness. He's demonstrating exactly the kind of careful, honest thinking you want from someone in his position. This pattern isn't new. In fact, it's how every major scientific breakthrough has worked. When nuclear physicists first split the atom, they didn't pretend to understand all the implications. They were remarkably honest about the unknowns, and that honesty actually helped society navigate one of the most dangerous technologies ever created. When climate scientists first started warning about global warming, they didn't claim perfect prediction models. They said, "We see clear trends, but there's uncertainty in the exact timing and magnitude." Their willingness to acknowledge uncertainty while still communicating real risks actually strengthened their credibility over decades. When genomics researchers completed the Human Genome Project, they didn't claim they'd solved all of biology. They said, "This is a foundation, but we've learnt that biology is far more complex than we thought." In every case, acknowledging uncertainty alongside knowledge led to better outcomes, not worse ones. The experts who earn long-term trust are the ones who clearly communicate both what they know and what they don't know. So what does this mean for you? How do you apply this insight in your own life? First, start recognizing intellectual humility as a signal of expertise, not inexpertise. When someone admits uncertainty about complex topics, that's often a sign they actually understand the complexity. Second, become suspicious of false confidence. If someone claims to have simple answers to obviously complex questions, that's a red flag. Third, practice intellectual humility yourself; in your work, in your relationships, in your own learning. Get comfortable saying, "I don't know, but here's what I do know and here's what I'm uncertain about." The goal isn't to become paralyzed by uncertainty. The goal is to make better decisions by clearly distinguishing between what you know and what you don't. Because here's what the AI experts have figured out: admitting uncertainty doesn't make you weak, it makes you wise. And that brings us to why this matters for this entire series. Nobody Knows isn't about celebrating ignorance; it's about celebrating intellectual integrity. Over the next episodes, we're going to explore AI capabilities, limitations, and implications, but we're going to do it the way the best experts do it; by clearly distinguishing between what we know and what we don't know. Next episode, we'll dive into AI capabilities that actually work today versus the ones that are still science fiction. You might be surprised by where the line falls. So here's what we've learned today: the world's leading AI experts are remarkably uncertain about AI's future, and that's exactly what we should want them to be. But you know what? After researching this episode, I realize there's still so much I don't know about how this uncertainty affects you personally. How do you navigate AI tools when even the experts are uncertain? How do you make decisions in your work and life when the technology is changing so fast? Those are the questions we'll keep exploring together. Because the smartest people in the room just admitted they're figuring it out as they go, and that means you're in much better company than you thought.
