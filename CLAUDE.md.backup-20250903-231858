# CLAUDE.md - AI Podcast Production Master System üéì

<!-- markdownlint-disable-file -->

<!-- CLAUDE 4 OPTIMIZED: Token budget 15K, Project-specific patterns enabled -->
<!-- IMPROVED: August 2025 - LangGraph optimized with detected commands and patterns -->
<MANDATORY_CONTEXT>
<!-- This block MUST be loaded for all operations -->

## üéØ LANGGRAPH AI ORCHESTRATION SYSTEM

<SYSTEM_DIRECTIVE priority="MAXIMUM">
**PRODUCTION-READY: LangGraph-based multi-agent podcast production with 16 specialized agents**
</SYSTEM_DIRECTIVE>

**System Architecture (DETECTED):**
- **Development Mode**: Claude Code builds and tests LangGraph components
- **Production Mode**: LangGraph orchestration via `python main.py --topic "[TOPIC]" --budget [BUDGET]`
- **Monitoring Mode**: Real-time dashboard via `npm run dashboard`
- **Quality Mode**: Multi-evaluator consensus with brand voice validation

**Teaching Format:**
- **Technical:** Professional explanation with industry terminology  
- **Simple:** "Think of it like..." analogy-based explanation
- **Connection:** "This helps you learn..." learning value and transferable skills

**LangGraph Production Principles (August 2025):**
- **Graph-first Design**: State-based workflow orchestration
- **Minimal Shared State**: Immutable PodcastState TypedDict patterns
- **Built-in Persistence**: Database-backed checkpointing for reliability
- **Error Boundaries**: Per-agent error handling with graceful recovery
- **Cost Control**: Real-time budget tracking with automatic enforcement

## üìÖ MANDATORY TEMPORAL CONTEXT ENFORCEMENT

<CRITICAL_CONSTRAINT priority="MAXIMUM" override="NEVER">
**SYSTEM DATE: SEPTEMBER 2025 - NO TRAINING DATA POLICY ENFORCED**

**TRAINING DATA CUTOFF:** October 2024 - ALL technical knowledge is OUTDATED
**VALIDATION REQUIRED:** Every technical decision must use current sources

**MANDATORY VALIDATION PROTOCOL:**
- **BEFORE any technical implementation** ‚Üí Search current patterns with Perplexity MCP
- **BEFORE any library usage** ‚Üí Verify September 2025 compatibility via web search
- **BEFORE any architecture decision** ‚Üí Validate against current best practices
- **PROHIBITED:** Relying on training data for ANY technical implementation

**REQUIRED TOOLS for Validation:**
- **Perplexity MCP:** `mcp__perplexity-ask__perplexity_ask` for technical patterns
- **Web Search:** `WebSearch` for current documentation and standards
- **Both Required:** Use BOTH tools for critical technical decisions

**ENFORCEMENT RULES:**
- **Current Date:** September 4, 2025
- **All Web Searches:** MUST include "September 2025" or "2025" in queries
- **All Technical Decisions:** Based on September 2025 best practices and standards
- **All Documentation:** Reference September 2025 as current date
- **All API Versions:** Use latest versions available as of September 2025
- **All Frameworks:** Use September 2025 versions and patterns

**Search Query Template:**
```
"[TOPIC] September 2025 best practices current implementation"
"[TECHNOLOGY] 2025 latest patterns production ready"
"[FRAMEWORK] September 2025 version features documentation"
```

**VIOLATION CONSEQUENCES:**
- Using outdated training data patterns ‚Üí Work immediately rejected
- Implementing without validation ‚Üí Must redo with current sources
- Failing to specify September 2025 in searches ‚Üí Must redo search
- Using deprecated patterns ‚Üí Code rejected until updated
- **ZERO TOLERANCE:** No exceptions or bypasses allowed
</CRITICAL_CONSTRAINT>

## üîí CRITICAL PRODUCTION CONFIGURATION GOVERNANCE

<CRITICAL_CONSTRAINT override="NEVER">
**VOICE ID CHANGES REQUIRE EXPLICIT USER PERMISSION - NO EXCEPTIONS**
</CRITICAL_CONSTRAINT>

**CURRENT PRODUCTION VOICE:** ZF6FPAbjXT4488VcRRnw (Amelia - Episode 1 validated)

**Central Configuration:**
- `podcast_production/config/production-voice.json` - Single source of truth
- Environment variable: PRODUCTION_VOICE_ID=ZF6FPAbjXT4488VcRRnw
- All scripts reference central config, never hardcode

**Violation Consequences:**
- ANY unauthorized voice ID change immediately stops all work
- All work with wrong voice ID is invalidated
- Must restore correct voice ID before continuing

## üöÄ QUICK START NAVIGATION

**üòµ Feeling Overwhelmed?** ‚Üí `README.md` ‚Üí Simple 5-minute overview
**üèóÔ∏è Want to Understand?** ‚Üí `docs/ARCHITECTURE.md` ‚Üí How this sophisticated system works
**üö∂ First Episode?** ‚Üí `cd podcast_production && python main.py --topic "Why do we dream?" --dry-run`
**üîÑ Production Ready?** ‚Üí `python podcast_production/validate_production_readiness.py`
**üö® Need Help?** ‚Üí `docs/troubleshooting/` directory

## üìö PROJECT-SPECIFIC CONTEXT MANAGEMENT

**CONTEXT OPTIMIZATION FOR AI PODCAST PRODUCTION (15K TOKEN BUDGET)**

**Context Management Rules:**
- **Maximum Focus**: LangGraph workflows and agent coordination patterns
- **Single Source Truth**: Each production concern covered in exactly ONE location
- **Usage Documentation**: Every context reference must have clear operational purpose
- **Performance Optimization**: Selective loading based on current production phase

**Streamlined Context Architecture:**
```yaml
core_production:
  - podcast_production/main.py (CLI entry point and workflow orchestration)
  - podcast_production/workflows/main_workflow.py (LangGraph StateGraph definition)
  - podcast_production/core/state.py (PodcastState schema and management)
  - podcast_production/agents/ (16 specialized production agents)

configuration_management:
  - podcast_production/config/ (Production configuration centralization)
  - requirements.txt (Python dependencies)
  - package.json (Node.js dashboard dependencies)

quality_monitoring:
  - tests/integration/ (End-to-end workflow validation)
  - tests/quality_gates/ (Brand voice and quality validation)
  - dashboard/ (Real-time production monitoring)

documentation:
  - docs/reports/ (Production validation and assessment reports)
  - docs/deployment/ (Production deployment procedures)
  - episodes/production/ (Episode archives and metrics)
```

<CONTEXT_LOADING_PROTOCOL token_budget="15K_MAXIMUM">
**Context Loading Strategy:**
- **Entry Point:** This CLAUDE.md file - project navigation hub
- **Production Focus:** Load LangGraph workflows and agent patterns first
- **Quality Emphasis:** Include brand voice and cost control patterns
- **Token Priority:** PRODUCTION > QUALITY > MONITORING > DOCUMENTATION
- **Budget Enforcement:** Stop loading if approaching 15K token limit
- **Validation:** Every reference must justify relevance to current production task
</CONTEXT_LOADING_PROTOCOL>

## üìç CURRENT STATUS

**Phase:** Production System (Architecture Stabilized)
**LangGraph Migration:** 16/16 agents complete (100%)
**System Mode:** Full production readiness validated
**Production Stats:** 125+ episodes, $5.51 average cost per episode
**Quality Standards:** Multi-evaluator consensus operational (8.0+ targets)

<SYSTEM_COMMANDS priority="HIGH_FREQUENCY">
## üîß DETECTED PROJECT COMMANDS

**Production Workflow (Main System):**
- **Episode Production**: `cd podcast_production && python main.py --topic "[TOPIC]" --budget 5.51`
- **Dry Run Test**: `python main.py --topic "[TOPIC]" --dry-run`
- **Research Only**: `python main.py --research-only --topic "[TOPIC]"`
- **Verbose Mode**: `python main.py --topic "[TOPIC]" --verbose`

**Quality & Validation:**
- **Test Suite**: `pytest tests/ --verbose`
- **Production Readiness**: `python podcast_production/validate_production_readiness.py`
- **Health Check**: `python podcast_production/check_health.py`
- **Integration Tests**: `pytest tests/integration/ --verbose`

**Dashboard & Monitoring:**
- **Real-time Dashboard**: `npm run dashboard`
- **Start Dashboard**: `npm run dashboard-start` 
- **Test Dashboard**: `npm run dashboard-test`

**Quality Gates:**
- **Brand Voice**: `./tests/quality_gates/test_brand_voice_gates.sh`
- **Readability**: `./tests/quality_gates/test_readability_accessibility_gates.sh`
- **Dual Evaluation**: `./tests/quality_gates/test_dual_evaluation_consensus.sh`

**Cost & Performance:**
- **Cost Integration**: `./episodes/test_cost_integration.sh`
- **Framework Test**: `./tests/test_framework.sh`
- **Unit Tests**: `./tests/unit/test_[agent_name].sh`

**Context Management:**
- `/init` - Initialize project memory
- `/clear` - Clear conversation (use frequently!)
- `# note` - Quick memory addition

**Thinking Modes:**
- `think` - Basic reasoning
- `think hard` - Enhanced analysis (recommended)
- `ultrathink` - Maximum thinking (complex problems)
</SYSTEM_COMMANDS>

## üîí SECURITY CONFIGURATION

**GitHub Integration:**
PAT stored in `.env` file (git-ignored)
Usage: `source .env && git push origin main`

**MCP Environment Setup (VALIDATED):**
```bash
# ElevenLabs MCP (Working)
claude mcp list  # Verify ‚úì Connected status

# Test API connectivity
curl -H "xi-api-key: YOUR_KEY" https://api.elevenlabs.io/v1/models

# Environment variables loaded via .env in podcast_production/
```

**Production API Keys (Required):**
- ElevenLabs: Audio synthesis and voice cloning
- Claude: Script evaluation and quality assessment  
- Gemini: Alternative evaluation and consensus scoring
- Perplexity: Research validation and fact-checking

## üìÅ PROJECT-SPECIFIC DIRECTORY ARCHITECTURE

**AI PODCAST PRODUCTION SYSTEM - LANGGRAPH STRUCTURE (DETECTED)**

**Current Structure (v1.0.0 Production Standard):**
```
/
‚îú‚îÄ‚îÄ podcast_production/        # LangGraph Production System
‚îÇ   ‚îú‚îÄ‚îÄ agents/               # 16 Specialized Agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_discovery.py    # Initial topic research
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_deep_dive.py    # Comprehensive investigation  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_validation.py   # Fact-checking and verification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_synthesis.py    # Knowledge consolidation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ question_generator.py    # Strategic question creation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ episode_planner.py       # Episode structure planning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ script_writer.py         # Content creation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ brand_validator.py       # Brand voice consistency
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude_evaluator.py      # Quality assessment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_evaluator.py      # Alternative evaluation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio_synthesizer.py     # TTS and audio production
‚îÇ   ‚îú‚îÄ‚îÄ workflows/            # LangGraph Orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main_workflow.py         # Primary StateGraph definition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_pipeline.py     # Research workflow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ production_pipeline.py   # Content production workflow
‚îÇ   ‚îú‚îÄ‚îÄ core/                # System Infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state.py                 # PodcastState TypedDict
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state_manager.py         # State persistence & checkpointing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cost_tracker.py          # Budget enforcement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_orchestrator.py    # Agent coordination
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring.py           # Production monitoring
‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration Management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ production-voice.json   # Voice configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.yaml             # System settings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ providers.yaml          # API provider settings
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # CLI Entry Point
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ dashboard/               # Node.js Real-time Dashboard  
‚îÇ   ‚îú‚îÄ‚îÄ components/          # UI components
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Dashboard utilities
‚îú‚îÄ‚îÄ episodes/               # Production Archives
‚îÇ   ‚îî‚îÄ‚îÄ production/         # Episode data (ep001-ep125+)
‚îú‚îÄ‚îÄ tests/                 # Comprehensive Test Suite
‚îÇ   ‚îú‚îÄ‚îÄ integration/       # End-to-end workflow tests
‚îÇ   ‚îú‚îÄ‚îÄ quality_gates/     # Quality validation tests  
‚îÇ   ‚îî‚îÄ‚îÄ unit/             # Component unit tests
‚îú‚îÄ‚îÄ docs/                 # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ reports/          # Assessment reports
‚îÇ   ‚îî‚îÄ‚îÄ deployment/       # Production guides
‚îî‚îÄ‚îÄ Root (‚â§6 files): README.md, CLAUDE.md, requirements.txt,
                      package.json, pyproject.toml, LICENSE
```

**Directory Governance Rules:**
- **Production Focus**: All LangGraph components in `podcast_production/`
- **Agent Isolation**: Each agent as separate module with clear boundaries  
- **State Centralization**: All state management in `core/` directory
- **Configuration Single Source**: All configs in `config/` directory
- **Test Comprehensiveness**: Unit, integration, and quality gate coverage
- **Documentation Organization**: Reports, guides, and deployment docs separated

## üèóÔ∏è LANGGRAPH AI ORCHESTRATION ARCHITECTURE

**üö® PRODUCTION-READY:** Advanced LangGraph-based multi-agent orchestration (September 2025 patterns)

**Architecture Overview (DETECTED & VALIDATED):**
```
Claude Code (Development & Orchestration)
    ‚Üì builds and orchestrates
LangGraph Production System (podcast_production/)
    ‚îú‚îÄ‚îÄ StateGraph Workflows (main_workflow.py)
    ‚îú‚îÄ‚îÄ 16 Specialized Agents (research ‚Üí script ‚Üí audio)
    ‚îú‚îÄ‚îÄ Multi-Provider Adapters (ElevenLabs, Claude, Gemini, Perplexity)
    ‚îú‚îÄ‚îÄ Cost Tracking & Budget Enforcement
    ‚îî‚îÄ‚îÄ Quality Gates & Brand Validation
    ‚Üì outputs
Episode Production (125+ episodes, $5.51 average cost)
```

**LangGraph Production System (CURRENT IMPLEMENTATION):**
- **State Management**: PodcastState TypedDict with immutable patterns
- **Persistence**: Database-backed checkpointing with automatic recovery
- **Cost Control**: Real-time budget tracking with automatic enforcement  
- **Quality Assurance**: Multi-evaluator consensus scoring (8.0+ targets)
- **Observability**: Comprehensive logging and monitoring via dashboard
- **Error Handling**: Robust retry logic with graceful degradation
- **Agent Coordination**: 16 specialized agents with clear state boundaries

**September 2025 LangGraph Best Practices (IMPLEMENTED):**
- **Graph-first Design**: Clear state schema and decision flow mapping
- **Minimal Shared State**: Tightly scoped PodcastState to prevent bloat
- **Built-in Checkpointing**: Database-backed persistence for reliability
- **Error Boundaries**: Per-node error handling with termination conditions
- **Production Monitoring**: Comprehensive logging and tracing via dashboard
- **Human-in-the-Loop**: Review points for critical quality decisions
- **Container Deployment**: Docker-ready with environment configuration
- **Async Orchestration**: Full async/await patterns for performance

**Multi-Agent Workflow Patterns (PRODUCTION-TESTED):**
1. **Research Pipeline**: Discovery ‚Üí Deep Dive ‚Üí Validation ‚Üí Synthesis (4 agents)
2. **Content Pipeline**: Question Generation ‚Üí Planning ‚Üí Script Writing ‚Üí Brand Validation (4 agents)  
3. **Quality Pipeline**: Claude Evaluation ‚Üí Gemini Evaluation ‚Üí Consensus ‚Üí Final Polish (4 agents)
4. **Production Pipeline**: Audio Synthesis ‚Üí Validation ‚Üí Cost Tracking ‚Üí Output (4 agents)

## üîÑ WORKFLOW PROTOCOLS

**LangGraph StateGraph Orchestration:**
- **Entry Point**: `main.py` CLI with topic and budget parameters
- **State Flow**: Immutable PodcastState transitions through agent network
- **Checkpointing**: Automatic state persistence at each agent completion
- **Error Recovery**: Automatic retry with state rollback on failures
- **Cost Enforcement**: Real-time budget monitoring with automatic stopping

**Quality Assurance Pipeline:**
- **Multi-Evaluator Consensus**: Claude + Gemini + Brand validator scoring
- **Brand Voice Consistency**: Automated intellectual humility validation
- **Readability Standards**: Automated accessibility and engagement scoring
- **Production Standards**: Audio quality and synthesis validation

## üìä PRODUCTION SYSTEM STATUS

**System Validation**: ‚úÖ **PRODUCTION CERTIFIED** (September 2025)
**Architecture Status**: LangGraph migration 100% complete (16/16 agents)
**Production Performance**: 125+ episodes produced, $5.51 average cost
**Quality Standards**: Multi-evaluator consensus >8.0/10 maintained
**Cost Performance**: 90%+ episodes under $6.00 budget
**Reliability**: 95%+ success rate with automatic error recovery

**Latest Production Metrics:**
- ‚úÖ All MCP connections operational and tested
- ‚úÖ LangGraph StateGraph workflows validated
- ‚úÖ Brand voice consistency >85% maintained
- ‚úÖ Cost tracking accuracy validated
- ‚úÖ Quality gates operational with consensus scoring

## üéØ PROJECT OVERVIEW

**Mission:** Learn AI orchestration by building automated podcast production system
**Philosophy:** Intellectual humility - celebrating what we know AND what we don't
**Architecture:** LangGraph-based multi-agent orchestration with 16 specialized agents
**Technology Stack:** Python + LangGraph + Node.js dashboard + Multi-AI provider integration
**Production Status:** 125+ episodes produced, $5.51 average cost (vs traditional $800-3500)
**Quality Standards:** Multi-evaluator consensus (8.0+ targets), brand voice consistency >85%
**Learning Emphasis:** Every step teaches transferable AI orchestration and LangGraph patterns

**Key Technologies (DETECTED & VALIDATED):**
- **Orchestration**: LangGraph 0.2+ with StateGraph and checkpointing
- **Agents**: 16 specialized agents with clear state boundaries
- **Monitoring**: Node.js dashboard with WebSocket real-time updates
- **APIs**: ElevenLabs, Claude, Gemini, Perplexity via MCP servers  
- **State Management**: TypedDict with immutable patterns and persistence
- **Cost Tracking**: Built-in budget enforcement with real-time monitoring
- **Quality Gates**: Multi-dimensional validation with consensus scoring
- **Deployment**: Docker-ready with production environment configuration

## üéØ CURRENT PRIORITIES (SEPTEMBER 2025)

1. **Production Optimization**:
   - Maintain $5.51 average cost per episode
   - Scale to 2-3 episodes per week production
   - Optimize agent coordination for faster workflows

2. **Quality Enhancement**:
   - Maintain >85% brand voice consistency  
   - Improve multi-evaluator consensus accuracy
   - Enhance audio synthesis quality metrics

3. **System Reliability**:
   - Maintain >95% production success rate
   - Optimize error recovery and checkpoint systems
   - Enhance monitoring and observability

4. **Learning Documentation**:
   - Document LangGraph patterns for educational value
   - Create transferable AI orchestration examples
   - Maintain comprehensive production reports

## üí° PRO TIPS

- **Start with Dry Run**: Always test with `--dry-run` before production
- **Monitor Costs**: Budget enforcement stops workflows at limits
- **Use Dashboard**: Real-time monitoring via `npm run dashboard`
- **Quality First**: Multi-evaluator consensus ensures brand consistency
- **Learn Patterns**: Every workflow teaches transferable LangGraph skills

## üé™ REMEMBER

This is YOUR learning journey - go at YOUR pace!
LangGraph orchestration > Simple chains in 2025.
Every episode teaches valuable AI orchestration patterns.
Production readiness comes from systematic quality gates.

---

**Quick Actions:** 
- **New Episode**: `cd podcast_production && python main.py --topic "[TOPIC]" --budget 5.51`
- **Dashboard**: `npm run dashboard`
- **Health Check**: `python podcast_production/check_health.py`
- **Documentation**: `docs/ARCHITECTURE.md`

**Version:** 9.0.0 | **Updated:** 2025-09-04 | **Architecture:** LangGraph Production | **Status:** 100% Complete