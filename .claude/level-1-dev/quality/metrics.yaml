# Quality Metrics Definition for Level-1-Dev
# Specific, measurable indicators of quality

version: "1.0.0"
last_updated: "2025-01-16"

# Core Metrics
metrics:
  # Reliability Metrics
  reliability:
    test_pass_rate:
      description: "Percentage of tests passing"
      formula: "(passed_tests / total_tests) * 100"
      target: 95
      minimum: 90
      data_source: "test-runner output"
      frequency: "per-commit"

    error_rate:
      description: "Errors per 1000 executions"
      formula: "(error_count / total_executions) * 1000"
      target: 5
      maximum: 10
      data_source: "execution logs"
      frequency: "daily"

    mean_time_between_failures:
      description: "Average time between failures (hours)"
      formula: "total_operational_hours / failure_count"
      target: 168  # 1 week
      minimum: 72  # 3 days
      data_source: "monitoring logs"
      frequency: "weekly"

    recovery_time:
      description: "Average time to recover from failure (minutes)"
      formula: "sum(recovery_times) / failure_count"
      target: 5
      maximum: 15
      data_source: "incident logs"
      frequency: "per-incident"

  # Maintainability Metrics
  maintainability:
    code_complexity:
      description: "Cyclomatic complexity of scripts"
      formula: "decision_points + 1"
      target: 5
      maximum: 10
      data_source: "static analysis"
      frequency: "per-commit"

    documentation_coverage:
      description: "Percentage of documented functions"
      formula: "(documented_functions / total_functions) * 100"
      target: 90
      minimum: 80
      data_source: "documentation scanner"
      frequency: "weekly"

    duplicate_code_ratio:
      description: "Percentage of duplicated code"
      formula: "(duplicate_lines / total_lines) * 100"
      target: 5
      maximum: 10
      data_source: "duplication detector"
      frequency: "per-commit"

    technical_debt_ratio:
      description: "Ratio of remediation cost to development cost"
      formula: "remediation_effort / development_effort"
      target: 0.05
      maximum: 0.10
      data_source: "debt tracker"
      frequency: "sprint"

  # Testability Metrics
  testability:
    test_coverage:
      description: "Line coverage percentage"
      formula: "(tested_lines / total_lines) * 100"
      target: 80
      minimum: 70
      data_source: "coverage tool"
      frequency: "per-commit"

    test_effectiveness:
      description: "Bugs found by tests vs production"
      formula: "test_bugs / (test_bugs + production_bugs)"
      target: 0.90
      minimum: 0.80
      data_source: "bug tracker"
      frequency: "monthly"

    test_execution_time:
      description: "Total test suite execution time (seconds)"
      formula: "sum(test_durations)"
      target: 60
      maximum: 120
      data_source: "test runner"
      frequency: "per-run"

    assertion_density:
      description: "Assertions per test function"
      formula: "total_assertions / test_function_count"
      target: 3
      minimum: 2
      data_source: "test analyzer"
      frequency: "per-commit"

  # Security Metrics
  security:
    vulnerability_count:
      description: "Number of known vulnerabilities"
      formula: "critical + high + medium"
      target: 0
      maximum: 0
      data_source: "security scanner"
      frequency: "daily"

    security_issue_resolution_time:
      description: "Time to fix security issues (hours)"
      formula: "sum(fix_times) / issue_count"
      target: 24
      maximum: 48
      data_source: "issue tracker"
      frequency: "per-issue"

    unsafe_pattern_violations:
      description: "Count of unsafe coding patterns"
      formula: "sum(pattern_violations)"
      target: 0
      maximum: 3
      data_source: "pattern scanner"
      frequency: "per-commit"

    secret_exposure_incidents:
      description: "Accidental secret commits"
      formula: "count(secret_commits)"
      target: 0
      maximum: 0
      data_source: "secret scanner"
      frequency: "per-commit"

  # Performance Metrics
  performance:
    execution_speed:
      description: "Average script execution time (ms)"
      formula: "sum(execution_times) / execution_count"
      target: 100
      maximum: 500
      data_source: "performance monitor"
      frequency: "per-run"

    memory_usage:
      description: "Peak memory consumption (MB)"
      formula: "max(memory_samples)"
      target: 50
      maximum: 100
      data_source: "resource monitor"
      frequency: "per-run"

    cpu_efficiency:
      description: "CPU utilization percentage"
      formula: "(cpu_time / wall_time) * 100"
      target: 80
      minimum: 60
      data_source: "resource monitor"
      frequency: "per-run"

    io_operations:
      description: "File I/O operations per execution"
      formula: "read_ops + write_ops"
      target: 10
      maximum: 50
      data_source: "io monitor"
      frequency: "per-run"

# Composite Metrics
composite_metrics:
  overall_quality_score:
    description: "Weighted average of all dimensions"
    components:
      - metric: reliability.test_pass_rate
        weight: 0.25
      - metric: maintainability.documentation_coverage
        weight: 0.20
      - metric: testability.test_coverage
        weight: 0.20
      - metric: security.vulnerability_count
        weight: 0.20
      - metric: performance.execution_speed
        weight: 0.15
    formula: "weighted_average(components)"
    target: 85
    minimum: 75

  release_readiness:
    description: "Ready for production release"
    requirements:
      - test_pass_rate: ">= 95"
      - vulnerability_count: "== 0"
      - documentation_coverage: ">= 90"
      - test_coverage: ">= 80"
    formula: "all_requirements_met"
    target: true

  technical_health:
    description: "Overall technical health indicator"
    components:
      - code_complexity: "< 10"
      - duplicate_code_ratio: "< 10"
      - technical_debt_ratio: "< 0.10"
      - test_effectiveness: "> 0.80"
    formula: "percentage_meeting_thresholds"
    target: 90
    minimum: 75

# Trend Analysis
trends:
  calculation_period: "30_days"

  trend_metrics:
    - metric: overall_quality_score
      direction: improving  # improving, declining, stable
      threshold: 5  # % change to be significant

    - metric: test_coverage
      direction: improving
      threshold: 2

    - metric: vulnerability_count
      direction: stable
      threshold: 1

    - metric: execution_speed
      direction: improving
      threshold: 10

# Alerting Thresholds
alerts:
  critical:
    - condition: "vulnerability_count > 0"
      action: "immediate_notification"

    - condition: "test_pass_rate < 80"
      action: "block_deployment"

    - condition: "secret_exposure_incidents > 0"
      action: "security_review"

  warning:
    - condition: "test_coverage < 70"
      action: "developer_notification"

    - condition: "code_complexity > 15"
      action: "refactoring_required"

    - condition: "technical_debt_ratio > 0.15"
      action: "debt_review"

  info:
    - condition: "documentation_coverage < 85"
      action: "reminder_notification"

    - condition: "execution_speed > 300"
      action: "performance_review"

# Data Collection
collection:
  automated_tools:
    - name: "test-runner"
      metrics: ["test_pass_rate", "test_execution_time"]
      schedule: "on_commit"

    - name: "coverage-analyzer"
      metrics: ["test_coverage", "assertion_density"]
      schedule: "on_commit"

    - name: "security-scanner"
      metrics: ["vulnerability_count", "unsafe_pattern_violations"]
      schedule: "daily"

    - name: "performance-monitor"
      metrics: ["execution_speed", "memory_usage", "cpu_efficiency"]
      schedule: "on_demand"

  manual_collection:
    - name: "code-review"
      metrics: ["maintainability", "documentation_quality"]
      schedule: "per_pr"

    - name: "incident-analysis"
      metrics: ["mean_time_between_failures", "recovery_time"]
      schedule: "per_incident"

# Reporting Configuration
reporting:
  dashboards:
    executive:
      metrics:
        - overall_quality_score
        - release_readiness
        - vulnerability_count
      format: "summary"
      frequency: "weekly"

    developer:
      metrics:
        - test_pass_rate
        - test_coverage
        - code_complexity
        - execution_speed
      format: "detailed"
      frequency: "daily"

    quality:
      metrics: "*"  # All metrics
      format: "comprehensive"
      frequency: "sprint"

# Historical Tracking
history:
  retention_days: 90

  storage:
    location: "quality/metrics-history/"
    format: "json"
    compression: true

  snapshots:
    daily: true
    weekly: true
    monthly: true
