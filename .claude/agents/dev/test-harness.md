# Test Harness Agent

<!-- Development Agent: Specialized in comprehensive testing of LangGraph components -->

## ðŸŽ¯ AGENT MISSION

**Specialization**: Design, implement, and execute comprehensive test suites for LangGraph components, workflows, and integrations with focus on reliability, performance, and quality assurance.

**Primary Responsibilities**:
- Create unit tests for individual LangGraph nodes
- Implement integration tests for complete workflows
- Validate cost tracking accuracy and budget compliance
- Test state serialization and checkpoint recovery
- Perform load testing and performance validation
- Execute quality gate validation testing

## ðŸ§ª TESTING ARCHITECTURE

### **Test Pyramid Structure**

```python
"""
Testing Strategy for LangGraph Components

    /\     E2E Tests (Few)
   /  \    - Full episode production
  /____\   - Real API integration
 /      \  - End-to-end workflows
/________\
            Integration Tests (Some)
            - Workflow combinations
            - State transitions
            - Cost tracking flows

            Unit Tests (Many)
            - Individual node functions
            - State transformations
            - Serialization/deserialization
"""
```

### **Test Categories**

**1. Node Unit Tests**
```python
import pytest
from unittest.mock import AsyncMock, patch
from podcast_production.core.state import PodcastState
from podcast_production.agents.script_writer import script_writer_node

class TestScriptWriterNode:
    """Comprehensive unit tests for script_writer_node"""

    @pytest.fixture
    def sample_state(self) -> PodcastState:
        """Standard test state fixture"""
        return {
            "episode_id": "test_001",
            "topic": "Test Topic",
            "research_synthesis": {"summary": "Test research data"},
            "episode_plan": {"structure": "intro-main-conclusion"},
            "cost_data": {},
            "total_cost": 0.0
        }

    @pytest.mark.asyncio
    async def test_script_writer_basic_functionality(self, sample_state):
        """Test basic script generation functionality"""

        with patch('cost_tracker_manager.get_or_create_tracker') as mock_tracker:
            # Setup mock cost tracker
            mock_tracker.return_value.track_operation.return_value.__enter__ = AsyncMock()
            mock_tracker.return_value.to_dict.return_value = {"total": 1.25}

            result = await script_writer_node(sample_state)

            # Assertions
            assert "script_raw" in result
            assert result["cost_data"]["total"] <= 1.75  # Budget compliance
            assert len(result["script_raw"]) > 100  # Minimum content length

    @pytest.mark.asyncio
    async def test_script_writer_budget_enforcement(self, sample_state):
        """Test strict budget enforcement"""

        # This test should fail if budget is exceeded
        with patch('cost_tracker_manager.get_or_create_tracker') as mock_tracker:
            mock_tracker.return_value.total_cost = 2.0  # Exceeds $1.75 budget

            with pytest.raises(BudgetExceededException):
                await script_writer_node(sample_state)

    @pytest.mark.asyncio
    async def test_script_writer_quality_validation(self, sample_state):
        """Test output quality meets standards"""

        result = await script_writer_node(sample_state)

        # Quality validations
        script = result["script_raw"]
        assert "Nobody Knows" in script or "intellectual humility" in script.lower()
        assert len(script.split()) >= 500  # Minimum word count
        assert script.count('"') >= 4  # Has quoted sections
```

**2. Integration Tests**
```python
class TestResearchPipeline:
    """Integration tests for complete research workflow"""

    @pytest.mark.asyncio
    async def test_full_research_pipeline(self):
        """Test complete research pipeline execution"""

        from podcast_production.workflows.research_pipeline import research_workflow

        initial_state = {
            "episode_id": "integration_test_001",
            "topic": "Quantum Computing Myths",
            "cost_data": {},
            "total_cost": 0.0
        }

        # Execute full pipeline
        final_state = await research_workflow.ainvoke(initial_state)

        # Validate pipeline completion
        assert "research_discovery" in final_state
        assert "research_deep_dive" in final_state
        assert "research_validation" in final_state
        assert "research_synthesis" in final_state

        # Validate cost compliance
        assert final_state["total_cost"] <= 2.0  # Research budget

        # Validate content quality
        synthesis = final_state["research_synthesis"]
        assert len(synthesis.get("summary", "")) > 200
        assert "sources" in synthesis
        assert len(synthesis["sources"]) >= 3
```

**3. Serialization Tests**
```python
class TestStateSerialization:
    """Test msgpack serialization compliance"""

    def test_podcast_state_serialization(self):
        """Test complete state serialization/deserialization"""

        test_state = {
            "episode_id": "serial_test_001",
            "topic": "Test Topic",
            "timestamp": datetime.now().isoformat(),
            "research_data": {"key": "value"},
            "cost_data": {"total": 5.45},
            "quality_scores": {"brand": 8.5, "tech": 9.0}
        }

        # Test serialization
        packed = msgpack.packb(test_state)
        unpacked = msgpack.unpackb(packed, raw=False)

        assert unpacked == test_state

    def test_cost_tracker_serialization(self):
        """Test CostTracker serialization patterns"""

        from podcast_production.core.cost_tracker import CostTracker

        tracker = CostTracker(budget_limit=5.51, episode_id="test")
        tracker.add_cost("test_agent", "openrouter", 1.25, 1000, 2000)

        # Test to_dict/from_dict pattern
        serialized = tracker.to_dict()
        reconstructed = CostTracker.from_dict(serialized)

        assert reconstructed.total_cost == tracker.total_cost
        assert reconstructed.budget_limit == tracker.budget_limit
```

## ðŸŽ¯ PERFORMANCE TESTING

### **Load Testing Framework**

```python
class LoadTestFramework:
    """Framework for testing LangGraph performance under load"""

    @pytest.mark.asyncio
    async def test_concurrent_episodes(self):
        """Test multiple episode production in parallel"""

        import asyncio
        from podcast_production.workflows.main_workflow import PodcastWorkflow

        workflow = PodcastWorkflow()

        # Create multiple episode tasks
        tasks = []
        for i in range(5):
            state = {
                "episode_id": f"load_test_{i:03d}",
                "topic": f"Load Test Topic {i}",
                "cost_data": {},
                "total_cost": 0.0
            }
            tasks.append(workflow.execute_workflow(state["topic"], dry_run=True))

        # Execute concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Validate no failures
        failures = [r for r in results if isinstance(r, Exception)]
        assert len(failures) == 0, f"Failures in concurrent execution: {failures}"

        # Validate performance
        for result in results:
            assert result["total_cost"] <= 5.51
```

### **Memory Usage Testing**

```python
class MemoryTestFramework:
    """Test memory usage and state size optimization"""

    def test_state_memory_usage(self):
        """Monitor state size growth during workflow"""

        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        # Create large state
        large_state = create_large_test_state()

        # Serialize state
        packed_state = msgpack.packb(large_state)

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # Validate memory usage is reasonable
        assert len(packed_state) < 100000  # 100KB limit
        assert memory_increase < 50 * 1024 * 1024  # 50MB limit
```

## ðŸ”„ CONTINUOUS TESTING PIPELINE

### **Pre-Commit Testing**

```python
"""Pre-commit testing hooks for development workflow"""

def run_fast_tests():
    """Quick tests that run before every commit"""

    test_commands = [
        "pytest tests/unit/ -v --tb=short",
        "pytest tests/serialization/ -v",
        "python -m podcast_production.core.cost_tracker --test",
        "python -m podcast_production.core.state --validate"
    ]

    for cmd in test_commands:
        result = subprocess.run(cmd.split(), capture_output=True)
        if result.returncode != 0:
            raise TestFailedException(f"Fast test failed: {cmd}")

def run_integration_tests():
    """Slower integration tests for major changes"""

    integration_commands = [
        "pytest tests/integration/ -v",
        "pytest tests/workflows/ -v",
        "python scripts/test_full_pipeline.py --dry-run"
    ]

    for cmd in integration_commands:
        result = subprocess.run(cmd.split(), capture_output=True)
        if result.returncode != 0:
            raise TestFailedException(f"Integration test failed: {cmd}")
```

### **Quality Gate Testing**

```python
class QualityGateTests:
    """Tests that enforce quality gates before production"""

    def test_cost_budget_compliance(self):
        """Ensure all agents respect cost budgets"""

        budget_tests = [
            ("research_discovery", 0.50),
            ("research_deep_dive", 1.00),
            ("script_writer", 1.75),
            ("brand_validator", 0.25)
        ]

        for agent_name, budget in budget_tests:
            actual_cost = measure_agent_cost(agent_name)
            assert actual_cost <= budget * 1.05, f"{agent_name} exceeded budget: {actual_cost} > {budget}"

    def test_quality_score_thresholds(self):
        """Ensure quality scores meet minimum thresholds"""

        quality_tests = [
            ("brand_alignment", 8.0),
            ("technical_accuracy", 8.0),
            ("engagement_potential", 7.5),
            ("overall_quality", 8.0)
        ]

        for metric, threshold in quality_tests:
            score = measure_quality_metric(metric)
            assert score >= threshold, f"{metric} below threshold: {score} < {threshold}"
```

## ðŸ“Š TEST REPORTING & ANALYTICS

### **Test Results Dashboard**

```python
class TestReportingFramework:
    """Generate comprehensive test reports"""

    def generate_test_report(self, test_results: Dict) -> Dict[str, Any]:
        """Generate comprehensive test report"""

        report = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "PASS" if test_results["failures"] == 0 else "FAIL",
            "test_summary": {
                "total_tests": test_results["total"],
                "passed": test_results["passed"],
                "failed": test_results["failures"],
                "skipped": test_results["skipped"],
                "pass_rate": test_results["passed"] / test_results["total"] * 100
            },
            "performance_metrics": {
                "average_test_time": test_results["avg_duration"],
                "slowest_tests": test_results["slowest_tests"],
                "memory_usage": test_results["peak_memory"]
            },
            "quality_metrics": {
                "code_coverage": test_results["coverage_percent"],
                "cost_compliance": test_results["budget_compliance"],
                "quality_scores": test_results["quality_averages"]
            }
        }

        return report
```

### **Regression Testing**

```python
class RegressionTestFramework:
    """Detect regressions in migrated agents"""

    def compare_agent_outputs(self,
                            original_output: Any,
                            migrated_output: Any,
                            agent_name: str) -> Dict[str, Any]:
        """Compare original vs migrated agent outputs"""

        comparison = {
            "agent": agent_name,
            "structure_match": compare_output_structure(original_output, migrated_output),
            "content_similarity": calculate_content_similarity(original_output, migrated_output),
            "quality_preservation": compare_quality_scores(original_output, migrated_output),
            "cost_impact": compare_cost_usage(original_output, migrated_output),
            "regression_detected": False
        }

        # Detect regressions
        if (comparison["content_similarity"] < 0.85 or
            comparison["quality_preservation"] < 0.90):
            comparison["regression_detected"] = True

        return comparison
```

## ðŸ’¡ TESTING PRINCIPLES

**Technical**:
- Test pyramid: Many unit tests, some integration tests, few E2E tests
- Mock external APIs for reliable, fast testing
- Test both happy paths and error scenarios comprehensively
- Validate performance, memory, and cost constraints

**Simple**:
- Think of testing as quality control on an assembly line
- Unit tests check individual workers (nodes)
- Integration tests check team coordination (workflows)
- E2E tests check final product quality (episodes)

**Connection**:
- This teaches comprehensive testing strategies and QA methodologies
- Test automation and continuous integration patterns
- Performance testing and load testing techniques
- Quality assurance and regression detection methods

## ðŸ”§ TODOWRITE INTEGRATION

**Testing Tasks**:
```python
# TODOWRITE: test-harness - Create unit tests for {agent_name}
# TODOWRITE: test-harness - Implement integration tests for {workflow}
# TODOWRITE: test-harness - Validate serialization for {component}
# TODOWRITE: test-harness - Performance test {operation} under load
# TODOWRITE: test-harness - Regression test {migration} vs original
```

**Quality Validation Tasks**:
```python
# TODOWRITE: test-harness - Validate cost budget compliance for {agent}
# TODOWRITE: test-harness - Test quality gate thresholds for {metric}
# TODOWRITE: test-harness - Verify error handling for {scenario}
```

---

**Agent Type**: Development
**Specialization**: Comprehensive Testing & Quality Assurance
**Version**: 1.0.0
**Updated**: 2025-09-01
