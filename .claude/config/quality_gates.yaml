# Quality Gates Configuration for Nobody Knows Podcast Production
# Version: 4.0.0 - EMPIRICAL CALIBRATION BASED ON EPISODE 1 RESULTS
# Last Updated: 2025-08-25
# Purpose: Define quality thresholds and evaluation criteria for multi-agent production pipeline
#
# MAJOR UPDATE: Episode 1 Empirical Integration
# - Updated WPM calculation: 206 WPM (empirical) vs 150 WPM (assumption)
# - Added audio quality thresholds: 94.89% word accuracy, 91.23% character accuracy
# - Integrated cost targets: $2.77 per episode (empirical vs $6.30 assumption)
# - Added pronunciation accuracy validation for technical terms and expert names
#
# CONFIGURATION CENTRALIZATION:
# References production-voice.json for voice configuration (Single Source of Truth)
# Target duration: 28 minutes (25-30 min range)
# Target content: 5768 words (28 min * 206 WPM empirical rate)
# IMPORTANT: All thresholds now based on Episode 1 empirical validation

quality_gates:
  # Core quality thresholds (0.0 - 1.0 scale)
  thresholds:
    comprehension:
      minimum: 0.85              # Must exceed for publication
      target: 0.90               # Quality goal
      description: "General audience understanding without prior technical knowledge"
      weight: 0.20               # Adjusted for audio quality integration

    brand_consistency:
      minimum: 0.90              # Core brand requirement
      target: 0.95               # Brand excellence
      description: "Alignment with Nobody Knows brand voice and philosophy"
      weight: 0.30               # Highest importance - brand critical

    engagement:
      minimum: 0.80              # Listener retention requirement
      target: 0.85               # Engagement goal
      description: "Ability to maintain listener interest for full 28 minutes"
      weight: 0.20               # Audience experience

    technical_accuracy:
      minimum: 0.85              # Factual correctness requirement
      target: 0.92               # Technical excellence
      description: "Factual correctness and proper technical explanations"
      weight: 0.15               # Adjusted for audio quality integration

    audio_quality:
      minimum: 0.85              # Episode 1 baseline: 92.1% composite
      target: 0.92               # Episode 1 empirical performance
      description: "Speech synthesis quality including pronunciation and accuracy"
      weight: 0.15               # New empirical quality dimension from Episode 1

      sub_metrics:
        word_accuracy:
          minimum: 0.90           # Episode 1 achieved: 94.89%
          target: 0.95            # Stretch goal
          description: "STT validation - word recognition accuracy"

        character_accuracy:
          minimum: 0.85           # Episode 1 achieved: 91.23%
          target: 0.92            # Episode 1 empirical performance
          description: "Character-level transcription accuracy including punctuation"

        pronunciation_accuracy:
          minimum: 0.90           # Technical terms and expert names
          target: 0.95            # Production standard
          description: "Correct pronunciation of technical terms and proper names"
          critical_terms:
            - expert_names: ["Yoshua Bengio", "Geoffrey Hinton"]
            - technical_terms: ["OECD", "Turing", "algorithm"]
            - statistics: ["percentages", "numerical_data"]

  # Brand voice requirements
  brand_metrics:
    intellectual_humility:
      minimum_per_episode: 3            # Minimum acceptable
      target_per_episode: 5             # Per episode target
      examples:
        - "Nobody fully understands..."
        - "We're still learning..."
        - "Scientists are debating..."
        - "This might be wrong, but..."
        - "The honest answer is we don't know..."

    question_density:
      minimum_per_1000_words: 2         # Minimum engagement
      target_per_1000_words: 4          # Target curiosity questions
      types:
        - curiosity_prompts: "What if...?"
        - technical_questions: "How does this actually work?"
        - philosophical: "What does this mean for...?"
        - engagement: "Have you ever wondered...?"

    feynman_analogies:
      minimum_per_episode: 3            # Basic requirement
      target_per_episode: 5             # Clear explanations
      quality_criteria:
        - uses_everyday_objects: true
        - builds_progressively: true
        - avoids_nested_complexity: true

    fridman_curiosity:
      minimum_expressions: 2
      target_expressions: 4
      markers:
        - "fascinating"
        - "really interesting"
        - "makes you wonder"
        - "deeply curious about"

  # Episode structure requirements
  structure:
    duration:
      target_minutes: 28              # Episode target duration
      tolerance_minutes: 1            # Acceptable variance
      word_count_target: 5768         # Target word count (28 min * 206 WPM empirical rate)
      word_count_range: [5356, 6180]  # Min/max word counts based on 26-30 min * 206 WPM

    segments:
      introduction:
        duration_range: [1.0, 2.0]     # Introduction duration minutes
        required_elements:
          - hook
          - thesis_statement
          - episode_preview

      body:
        duration_range: [24.0, 26.0]   # Main body duration minutes
        required_elements:
          - foundation_concepts
          - progressive_complexity
          - practical_examples
          - intellectual_humility_moments

      conclusion:
        duration_range: [1.0, 2.0]     # Conclusion duration minutes
        required_elements:
          - key_takeaways
          - curiosity_prompt
          - next_episode_tease

  # Multi-model evaluation consensus
  evaluation_consensus:
    agreement_threshold: 0.10           # Good agreement threshold
    conflict_resolution:
      - strategy: "average"
        when: "difference < minor_threshold"
      - strategy: "conservative"
        when: "difference >= minor_threshold and < major_threshold"
        action: "use_lower_score"
      - strategy: "escalate"
        when: "difference >= major_threshold"
        action: "human_review"

    model_weights:
      claude: 0.55                        # Primary evaluator weight
      gemini: 0.45                        # Secondary evaluator weight

  # Revision thresholds
  revision_triggers:
    minor_revision:
      when: "1-2 gates fail by small margin"
      max_time_minutes: 10
      max_attempts: 2

    major_revision:
      when: "3+ gates fail OR any gate fails by significant margin"
      max_time_minutes: 20
      max_attempts: 3

    complete_rewrite:
      when: "any score below critical threshold OR technical_accuracy below minimum"
      action: "restart_from_planning"
      requires_approval: true

  # Cost controls - Updated with Episode 1 empirical results ($2.77)
  cost_limits:
    per_episode_maximum: 4.00           # Hard limit - pipeline halt (Episode 1: $2.77)
    per_episode_target: 2.80            # Realistic target based on Episode 1 empirical result
    revision_budget: 1.00               # Budget for revisions and optimization
    alert_threshold: 3.50               # Warning threshold

    agent_budgets:
      01_research_coordinator: 0.05      # Minimal LLM costs (research via web tools)
      02_episode_planner: 0.02           # Planning and coordination
      03_script_writer: 0.05             # Script generation
      04_quality_claude: 0.03            # Quality evaluation
      05_quality_gemini: 0.00            # Free tier
      06_feedback_synthesizer: 0.02      # Feedback processing
      07_script_polisher: 0.03           # TTS preparation
      08_final_reviewer: 0.02            # Final validation
      09_audio_synthesizer: 2.77         # ElevenLabs TTS (Episode 1 empirical cost)
      # Total: $2.99 allows for slight variance from Episode 1 result

  # Progressive complexity by season
  complexity_progression:
    season_1:
      episodes: [1, 25]
      complexity_range: [1, 3]
      focus: "fundamental_concepts"

    season_2:
      episodes: [26, 50]
      complexity_range: [3, 5]
      focus: "building_understanding"

    season_3:
      episodes: [51, 75]
      complexity_range: [5, 7]
      focus: "technical_depth"

    season_4:
      episodes: [76, 100]
      complexity_range: [7, 9]
      focus: "cutting_edge_research"

    season_5:
      episodes: [101, 125]
      complexity_range: [8, 10]
      focus: "philosophical_implications"

  # Failure handling
  failure_policies:
    max_total_attempts: 5
    timeout_minutes: 45

    escalation_path:
      - level: 1
        after_attempts: 2
        action: "increase_model_tier"
      - level: 2
        after_attempts: 3
        action: "alert_user"
      - level: 3
        after_attempts: 5
        action: "halt_pipeline"

    recovery_strategies:
      timeout: "save_progress_and_resume"
      quality_failure: "detailed_diagnostic_report"
      cost_overrun: "immediate_halt"
      api_error: "exponential_backoff"

  # Monitoring and metrics
  tracking:
    metrics_to_collect:
      - gate_pass_rates
      - revision_frequency
      - cost_per_episode
      - time_per_episode
      - quality_trend
      - brand_consistency_trend

    reporting:
      frequency: "after_each_episode"
      aggregate_reports: "weekly"
      quality_dashboard: true
      cost_dashboard: true

# Validation rules for this configuration
validation:
  all_thresholds_sum_to_1: true
  minimum_thresholds_below_targets: true
  cost_budgets_within_maximum: true
  complexity_ranges_progressive: true
