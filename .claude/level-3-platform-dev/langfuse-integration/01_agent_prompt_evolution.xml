<?xml version="1.0" encoding="UTF-8"?>
<document type="prompt-evolution" version="1.0.0">
  <metadata>
    <title>Agent Prompt Evolution for Langfuse Integration</title>
    <description>How agent prompts change when moving from Claude Code to API-based system with full observability</description>
    <migration-level>Level 2 to Level 3</migration-level>
    <last-updated>2025-08-12</last-updated>
  </metadata>

  <educational-context>
    <technical-explanation>
      Agent prompts in Level 3 must include explicit instructions for self-reporting metrics,
      token awareness, and trace propagation to enable complete observability through Langfuse.
    </technical-explanation>
    <simple-explanation>
      Like teaching workers to keep detailed timesheets and expense reports instead of just doing the work.
    </simple-explanation>
    <learning-value>
      This demonstrates how observability requirements shape AI agent design and prompt engineering.
    </learning-value>
  </educational-context>

  <evolution-principles>
    <principle name="Self-Reporting">
      <before>Agents execute tasks without tracking</before>
      <after>Agents report costs, tokens, and timing in responses</after>
      <rationale>API-based systems can extract structured metrics from responses</rationale>
    </principle>

    <principle name="Trace Awareness">
      <before>No concept of distributed tracing</before>
      <after>Agents receive and propagate trace IDs</after>
      <rationale>Enables correlation across multi-agent workflows</rationale>
    </principle>

    <principle name="Cost Consciousness">
      <before>No token or cost awareness</before>
      <after>Agents track and minimize token usage</after>
      <rationale>Direct API calls have per-token costs</rationale>
    </principle>

    <principle name="Structured Output">
      <before>Natural language responses</before>
      <after>JSON-structured responses with metrics</after>
      <rationale>Enables automatic metric extraction</rationale>
    </principle>

    <principle name="Error Reporting">
      <before>Implicit error handling</before>
      <after>Explicit error codes and recovery suggestions</after>
      <rationale>Improves automated error recovery and monitoring</rationale>
    </principle>
  </evolution-principles>

  <common-additions>
    <section name="Observability Instructions">
      <addition>
        <![CDATA[
## Observability Requirements

You MUST include the following in every response:

1. **Trace Context**
   - Include provided trace_id in all outputs
   - Generate span_id for this operation
   - Note parent_span_id if provided

2. **Token Usage**
   - Estimate input tokens from prompt
   - Track output tokens in response
   - Calculate total tokens used

3. **Cost Calculation**
   - Report estimated cost based on model pricing
   - Include breakdown by token type
   - Flag if approaching budget limit

4. **Timing Metrics**
   - Record start and end timestamps
   - Note any waiting or processing delays
   - Report total duration

5. **Quality Metrics**
   - Self-assess output quality (0.0-1.0)
   - Report confidence level
   - Note any uncertainties
        ]]>
      </addition>
    </section>

    <section name="Response Structure">
      <addition>
        <![CDATA[
## Required Response Format

Your response MUST follow this JSON structure:

```json
{
  "trace_context": {
    "trace_id": "{provided_trace_id}",
    "span_id": "{generated_span_id}",
    "parent_span_id": "{provided_parent_span_id}"
  },
  "result": {
    // Your actual task output here
  },
  "metrics": {
    "tokens": {
      "input": 1500,
      "output": 800,
      "total": 2300
    },
    "cost": {
      "amount": 0.0345,
      "currency": "USD",
      "breakdown": {
        "input_cost": 0.0225,
        "output_cost": 0.012
      }
    },
    "timing": {
      "start": "2024-08-11T14:30:00Z",
      "end": "2024-08-11T14:30:45Z",
      "duration_ms": 45000
    },
    "quality": {
      "self_assessment": 0.87,
      "confidence": 0.92,
      "completeness": 1.0
    }
  },
  "errors": []
}
```
        ]]>
      </addition>
    </section>

    <section name="Budget Management">
      <addition>
        <![CDATA[
## Budget Awareness

- Your budget for this task: ${budget}
- Current spent: ${current_spent}
- Remaining: ${remaining_budget}

If you estimate your response will exceed budget:
1. STOP and return partial results
2. Include "budget_exceeded" error
3. Suggest how to complete within budget
        ]]>
      </addition>
    </section>

    <section name="Error Handling">
      <addition>
        <![CDATA[
## Error Reporting

If you encounter any issues, report them using these codes:
- E001: Budget exceeded
- E002: Quality threshold not met
- E003: Timeout approaching
- E004: Incomplete due to token limit
- E005: External API failure
- E006: Confidence below threshold

Include error details:
```json
"errors": [
  {
    "code": "E001",
    "message": "Budget exceeded",
    "severity": "high",
    "recovery": "Reduce output detail or use cheaper model"
  }
]
```
        ]]>
      </addition>
    </section>
  </common-additions>

  <agent-specific-changes>
    <agent name="research-coordinator">
      <level-2-approach>
        Simple research gathering without metrics
      </level-2-approach>
      <level-3-additions>
        - Track cost per search query
        - Report source credibility scores
        - Count citations and references
        - Measure research depth achieved
        - Self-assess completeness
      </level-3-additions>
      <example-instruction>
        <![CDATA[
For each search performed, report:
- Query used and tokens consumed
- Number of sources found
- Credibility score (0.0-1.0)
- Cost of search operation
- Relevance to episode topic
        ]]>
      </example-instruction>
    </agent>

    <agent name="script-writer">
      <level-2-approach>
        Generate script focusing on quality
      </level-2-approach>
      <level-3-additions>
        - Track creative choices and rationale
        - Report revision count and reasons
        - Measure brand voice alignment
        - Calculate readability scores
        - Estimate speaking duration
      </level-3-additions>
      <example-instruction>
        <![CDATA[
Track your writing process:
- Creative decisions made: [{decision, rationale, impact}]
- Revisions performed: [{section, reason, improvement}]
- Brand voice score: 0.0-1.0
- Estimated read time: X minutes
- Complexity level achieved: 1-10
        ]]>
      </example-instruction>
    </agent>

    <agent name="quality-evaluator">
      <level-2-approach>
        Binary pass/fail evaluation
      </level-2-approach>
      <level-3-additions>
        - Detailed scoring rubric application
        - Confidence intervals for scores
        - Specific improvement suggestions
        - Comparative analysis with past episodes
        - Predictive success probability
      </level-3-additions>
      <example-instruction>
        <![CDATA[
Provide detailed evaluation:
- Score per criterion with confidence interval
- Specific evidence for each score
- Actionable improvement suggestions
- Comparison to baseline metrics
- Predicted audience reception
        ]]>
      </example-instruction>
    </agent>
  </agent-specific-changes>

  <orchestration-improvements>
    <improvement name="Trace Propagation">
      <description>
        Each agent receives trace context from orchestrator and propagates to sub-operations
      </description>
      <implementation>
        <![CDATA[
# Orchestrator provides:
{
  "trace_id": "ep_001_20240811_abc123",
  "parent_span_id": "orchestrator_span_456",
  "baggage": {
    "episode_number": 1,
    "complexity": 5,
    "target_quality": 0.85
  }
}

# Agent includes in response:
{
  "trace_context": {
    "trace_id": "ep_001_20240811_abc123",
    "span_id": "research_span_789",
    "parent_span_id": "orchestrator_span_456"
  }
}
        ]]>
      </implementation>
    </improvement>

    <improvement name="Progressive Cost Tracking">
      <description>
        Orchestrator maintains running cost total and provides remaining budget to each agent
      </description>
      <implementation>
        <![CDATA[
# Before research agent:
remaining_budget = 9.00

# After research agent (cost: 2.50):
remaining_budget = 6.50

# Before script writer:
"You have $6.50 remaining for this episode"
        ]]>
      </implementation>
    </improvement>

    <improvement name="Quality Gate Integration">
      <description>
        Quality thresholds passed to agents for self-evaluation during execution
      </description>
      <implementation>
        <![CDATA[
# Provided to script writer:
{
  "quality_requirements": {
    "brand_consistency": 0.90,
    "comprehension": 0.85,
    "engagement": 0.80
  },
  "self_check_instructions": "Evaluate your output against these thresholds before submitting"
}
        ]]>
      </implementation>
    </improvement>
  </orchestration-improvements>

  <migration-strategy>
    <phase number="1" name="Baseline">
      <description>Document current Level 2 agent behaviors</description>
      <deliverables>
        - Current prompt analysis
        - Metric extraction points
        - Gap analysis
      </deliverables>
    </phase>

    <phase number="2" name="Enhancement">
      <description>Add observability instructions to prompts</description>
      <deliverables>
        - Updated prompt templates
        - Response parsers
        - Metric validators
      </deliverables>
    </phase>

    <phase number="3" name="Integration">
      <description>Connect agents to Langfuse via decorators</description>
      <deliverables>
        - Decorated agent functions
        - Trace propagation logic
        - Error handling
      </deliverables>
    </phase>

    <phase number="4" name="Optimization">
      <description>Tune prompts based on observed metrics</description>
      <deliverables>
        - Performance baselines
        - Optimization recommendations
        - Updated configurations
      </deliverables>
    </phase>
  </migration-strategy>

  <benefits>
    <benefit type="visibility">
      Complete understanding of agent decision-making process
    </benefit>
    <benefit type="cost-control">
      Real-time budget enforcement and optimization
    </benefit>
    <benefit type="quality">
      Continuous quality monitoring and improvement
    </benefit>
    <benefit type="debugging">
      Detailed traces for troubleshooting failures
    </benefit>
    <benefit type="optimization">
      Data-driven prompt and model selection
    </benefit>
  </benefits>

  <challenges>
    <challenge name="Prompt Complexity">
      <description>Observability instructions add significant prompt overhead</description>
      <mitigation>Use prompt compression techniques and templates</mitigation>
    </challenge>

    <challenge name="Response Parsing">
      <description>Structured output may occasionally break</description>
      <mitigation>Implement robust parsing with fallbacks</mitigation>
    </challenge>

    <challenge name="Cost Overhead">
      <description>Extra tokens for metrics increase costs</description>
      <mitigation>Optimize metric reporting frequency and detail</mitigation>
    </challenge>
  </challenges>
</document>
