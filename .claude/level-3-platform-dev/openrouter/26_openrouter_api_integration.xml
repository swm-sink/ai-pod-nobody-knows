<?xml version="1.0" encoding="UTF-8"?>
<reference xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <metadata>
    <title>OpenRouter API Integration Guide</title>
    <type>reference</type>
    <version>2025.1</version>
    <last-updated>2025-01-11</last-updated>
    <domain>level-3-platform-dev</domain>
    <purpose>Technical implementation guide for OpenRouter API integration</purpose>
    <scope>Production-ready code patterns and best practices</scope>
    <sdk-compatibility>OpenAI SDK compatible</sdk-compatibility>
  </metadata>

  <executive-summary>
    <description>
      Comprehensive technical guide for integrating OpenRouter's unified AI model gateway into
      podcast production systems. Provides production-ready code patterns, error handling,
      cost optimization, and migration strategies from direct API implementations.
    </description>
  </executive-summary>

  <prerequisites>
    <technical-explanation>
      OpenRouter leverages OpenAI SDK compatibility, requiring only the standard OpenAI Python library
      with modified base URL configuration for transparent multi-provider access.
    </technical-explanation>
    <simple-explanation>
      You only need to install the regular OpenAI library and change one setting to access
      hundreds of AI models through OpenRouter.
    </simple-explanation>
    <installation>
      <command>pip install openai</command>
      <additional-dependencies>None required - uses standard OpenAI SDK</additional-dependencies>
    </installation>
  </prerequisites>

  <basic-configuration>
    <client-setup>
      <technical-explanation>
        Client initialization requires base URL override and optional headers for tracking
        and dashboard integration while maintaining OpenAI SDK interface compatibility.
      </technical-explanation>
      <simple-explanation>
        Set up the client by pointing it to OpenRouter instead of OpenAI, plus add some
        optional headers for better tracking and monitoring.
      </simple-explanation>
      <code-example>
        <![CDATA[
import os
from openai import OpenAI

# Initialize client with OpenRouter base URL
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    default_headers={
        "HTTP-Referer": "https://github.com/your-repo",  # Required
        "X-Title": "AI Podcast Nobody Knows"  # Optional, for dashboard
    }
)
        ]]>
      </code-example>
      <configuration-parameters>
        <parameter name="base_url">OpenRouter API endpoint</parameter>
        <parameter name="api_key">Environment variable for security</parameter>
        <parameter name="HTTP-Referer">Required header for request attribution</parameter>
        <parameter name="X-Title">Optional header for dashboard labeling</parameter>
      </configuration-parameters>
    </client-setup>
  </basic-configuration>

  <core-integration-patterns>
    <pattern id="research-agent">
      <title>Research Agent Integration</title>
      <technical-explanation>
        Research phase implementation using cost-optimized models with :floor routing
        for maximum savings during information gathering and initial synthesis.
      </technical-explanation>
      <simple-explanation>
        Use cheaper AI models for research tasks where you need good information but
        don't require the highest quality writing or reasoning.
      </simple-explanation>
      <implementation>
        <class-structure>
          <![CDATA[
class ResearchAgent:
    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )

    def gather_information(self, topic):
        """Use cost-optimized model for research"""
        try:
            response = self.client.chat.completions.create(
                model="mistralai/mixtral-8x7b-instruct:floor",  # Cost optimized
                messages=[
                    {"role": "system", "content": "You are a research assistant."},
                    {"role": "user", "content": f"Research: {topic}"}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            # Automatic failover handled by OpenRouter
            print(f"Research request handled with failover: {e}")
            return None
          ]]>
        </class-structure>
        <model-selection>
          <primary>mistralai/mixtral-8x7b-instruct:floor</primary>
          <rationale>Cost-optimized for research tasks</rationale>
          <fallback>Automatic through OpenRouter</fallback>
          <cost-impact>60% savings over premium models</cost-impact>
        </model-selection>
      </implementation>
    </pattern>

    <pattern id="script-generation">
      <title>Script Generation Integration</title>
      <technical-explanation>
        Script writing implementation leveraging premium models optimized for creativity
        and narrative coherence with higher temperature settings for natural dialogue.
      </technical-explanation>
      <simple-explanation>
        Use the best AI models for writing scripts since this is the content people will
        hearâ€”worth investing in quality for the core product.
      </simple-explanation>
      <implementation>
        <class-structure>
          <![CDATA[
class ScriptWriter:
    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )

    def generate_dialogue(self, research_content):
        """Use premium model for creative writing"""
        response = self.client.chat.completions.create(
            model="anthropic/claude-3-opus",  # Premium quality
            messages=[
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": research_content}
            ],
            max_tokens=4000,
            temperature=0.8
        )
        return response.choices[0].message.content

    def get_system_prompt(self):
        return """You are a podcast script writer creating engaging dialogue
        between two hosts exploring topics with intellectual humility."""
          ]]>
        </class-structure>
        <model-selection>
          <primary>anthropic/claude-3-opus</primary>
          <rationale>Premium quality for creative content</rationale>
          <temperature>0.8 for creative variation</temperature>
          <cost-impact>Higher cost justified by quality needs</cost-impact>
        </model-selection>
      </implementation>
    </pattern>

    <pattern id="quality-evaluation">
      <title>Quality Evaluator Integration</title>
      <technical-explanation>
        Quality assessment implementation using fast models with :nitro routing
        for rapid iteration cycles during validation and consistency checking.
      </technical-explanation>
      <simple-explanation>
        Use fast, accurate models for quality checking so you can quickly identify
        and fix problems without waiting for slow responses.
      </simple-explanation>
      <implementation>
        <class-structure>
          <![CDATA[
class QualityEvaluator:
    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )

    def evaluate_script(self, script):
        """Use fast model for quality checks"""
        response = self.client.chat.completions.create(
            model="openai/gpt-4-turbo:nitro",  # Speed optimized
            messages=[
                {"role": "system", "content": "Evaluate podcast script quality."},
                {"role": "user", "content": script}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        return self.parse_evaluation(response.choices[0].message.content)
          ]]>
        </class-structure>
        <model-selection>
          <primary>openai/gpt-4-turbo:nitro</primary>
          <rationale>Speed optimized for rapid feedback</rationale>
          <temperature>0.3 for consistent evaluation</temperature>
          <performance>Sub-second response times</performance>
        </model-selection>
      </implementation>
    </pattern>
  </core-integration-patterns>

  <advanced-features>
    <feature id="dynamic-model-selection">
      <title>Dynamic Model Selection</title>
      <technical-explanation>
        Algorithmic model selection based on task requirements and priority constraints,
        enabling optimization of cost-performance trade-offs across production workflow.
      </technical-explanation>
      <simple-explanation>
        Smart system that automatically picks the best AI model for each task based on
        whether you need speed, quality, or cost savings.
      </simple-explanation>
      <implementation>
        <![CDATA[
def select_model_for_task(task_type, priority="balanced"):
    """Dynamically select best model for task"""

    model_matrix = {
        "research": {
            "speed": "mistralai/mixtral-8x7b:nitro",
            "balanced": "anthropic/claude-instant",
            "quality": "anthropic/claude-3-sonnet"
        },
        "creative": {
            "speed": "openai/gpt-3.5-turbo",
            "balanced": "anthropic/claude-3-sonnet",
            "quality": "anthropic/claude-3-opus"
        },
        "evaluation": {
            "speed": "openai/gpt-3.5-turbo:nitro",
            "balanced": "openai/gpt-4-turbo",
            "quality": "openai/gpt-4"
        }
    }

    return model_matrix.get(task_type, {}).get(priority, "openai/gpt-3.5-turbo")
        ]]>
      </implementation>
      <selection-matrix>
        <task type="research">
          <speed>mistralai/mixtral-8x7b:nitro</speed>
          <balanced>anthropic/claude-instant</balanced>
          <quality>anthropic/claude-3-sonnet</quality>
        </task>
        <task type="creative">
          <speed>openai/gpt-3.5-turbo</speed>
          <balanced>anthropic/claude-3-sonnet</balanced>
          <quality>anthropic/claude-3-opus</quality>
        </task>
        <task type="evaluation">
          <speed>openai/gpt-3.5-turbo:nitro</speed>
          <balanced>openai/gpt-4-turbo</balanced>
          <quality>openai/gpt-4</quality>
        </task>
      </selection-matrix>
    </feature>

    <feature id="cost-tracking">
      <title>Cost Tracking and Monitoring</title>
      <technical-explanation>
        Real-time cost monitoring system tracking token usage and expenses per request
        with model-specific pricing integration for budget control and optimization.
      </technical-explanation>
      <simple-explanation>
        System that tracks exactly how much money you're spending on AI requests so you
        can stay within budget and see which parts cost the most.
      </simple-explanation>
      <implementation>
        <![CDATA[
class CostTracker:
    def __init__(self):
        self.costs = []

    def track_request(self, model, input_tokens, output_tokens):
        """Track costs per request"""
        # OpenRouter provides token counts in response
        cost = self.calculate_cost(model, input_tokens, output_tokens)
        self.costs.append({
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost
        })
        return cost

    def calculate_cost(self, model, input_tokens, output_tokens):
        """Calculate cost based on model pricing"""
        # Pricing would come from OpenRouter API or constants
        pricing = self.get_model_pricing(model)
        return (input_tokens * pricing["input"] +
                output_tokens * pricing["output"]) / 1000000
        ]]>
      </implementation>
      <tracking-capabilities>
        <capability>Real-time cost calculation per request</capability>
        <capability>Model-specific pricing integration</capability>
        <capability>Token usage monitoring and analysis</capability>
        <capability>Budget threshold alerts and warnings</capability>
      </tracking-capabilities>
    </feature>

    <feature id="error-handling">
      <title>Robust Error Handling</title>
      <technical-explanation>
        Comprehensive error handling with exponential backoff algorithms, retry logic,
        and graceful degradation patterns for production reliability.
      </technical-explanation>
      <simple-explanation>
        Smart error handling that tries again when things fail, waits longer between
        attempts if problems continue, and gives up gracefully if nothing works.
      </simple-explanation>
      <implementation>
        <![CDATA[
import time
from typing import Optional

class RobustClient:
    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )
        self.max_retries = 3
        self.base_delay = 1.0

    def make_request(self, **kwargs) -> Optional[str]:
        """Make request with exponential backoff"""
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(**kwargs)
                return response.choices[0].message.content
            except Exception as e:
                if attempt == self.max_retries - 1:
                    print(f"Failed after {self.max_retries} attempts: {e}")
                    return None

                delay = self.base_delay * (2 ** attempt)
                print(f"Attempt {attempt + 1} failed, retrying in {delay}s")
                time.sleep(delay)

        return None
        ]]>
      </implementation>
      <retry-strategy>
        <max-retries>3 attempts before giving up</max-retries>
        <backoff-algorithm>Exponential: 1s, 2s, 4s delays</backoff-algorithm>
        <failure-handling>Graceful degradation with None return</failure-handling>
        <logging>Detailed error tracking for debugging</logging>
      </retry-strategy>
    </feature>

    <feature id="streaming-responses">
      <title>Streaming Response Handling</title>
      <technical-explanation>
        Real-time streaming implementation for interactive applications with chunk-based
        processing and progressive response building for improved user experience.
      </technical-explanation>
      <simple-explanation>
        Get AI responses as they're being generated (like ChatGPT's typing effect)
        instead of waiting for the complete response, making the app feel faster.
      </simple-explanation>
      <implementation>
        <![CDATA[
def stream_generation(prompt, model="openai/gpt-3.5-turbo"):
    """Stream responses for real-time display"""
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )

    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_response += content
            print(content, end="", flush=True)

    return full_response
        ]]>
      </implementation>
      <streaming-benefits>
        <benefit>Improved perceived response time</benefit>
        <benefit>Real-time user interface updates</benefit>
        <benefit>Early termination capability</benefit>
        <benefit>Progressive content generation</benefit>
      </streaming-benefits>
    </feature>
  </advanced-features>

  <environment-configuration>
    <development-environment>
      <technical-explanation>
        Development configuration optimized for testing with cost-effective models
        and conservative token limits to prevent accidental overspending.
      </technical-explanation>
      <simple-explanation>
        Development settings that use cheaper models and smaller limits so you don't
        accidentally spend lots of money while testing your code.
      </simple-explanation>
      <configuration>
        <![CDATA[
# .env.development
OPENROUTER_API_KEY=sk-or-v1-dev-xxxxx
OPENROUTER_MAX_TOKENS=2000
OPENROUTER_DEFAULT_MODEL=openai/gpt-3.5-turbo
OPENROUTER_TEMPERATURE=0.7
        ]]>
      </configuration>
    </development-environment>

    <production-environment>
      <technical-explanation>
        Production configuration with higher token limits, premium default models,
        and failover enablement for maximum reliability and quality.
      </technical-explanation>
      <simple-explanation>
        Production settings that use better models with higher limits and backup
        systems enabled for the best quality and reliability.
      </simple-explanation>
      <configuration>
        <![CDATA[
# .env.production
OPENROUTER_API_KEY=sk-or-v1-prod-xxxxx
OPENROUTER_MAX_TOKENS=4000
OPENROUTER_DEFAULT_MODEL=anthropic/claude-3-sonnet
OPENROUTER_TEMPERATURE=0.8
OPENROUTER_FALLBACK_ENABLED=true
        ]]>
      </configuration>
    </production-environment>
  </environment-configuration>

  <testing-integration>
    <unit-testing>
      <technical-explanation>
        Unit test framework using mock objects to test integration logic without
        making actual API calls, enabling fast and reliable test execution.
      </technical-explanation>
      <simple-explanation>
        Testing your code without actually calling the real AI APIs by using fake
        responses, so tests run fast and don't cost money.
      </simple-explanation>
      <test-example>
        <![CDATA[
import unittest
from unittest.mock import patch, MagicMock

class TestOpenRouterIntegration(unittest.TestCase):
    @patch('openai.OpenAI')
    def test_research_agent(self, mock_client):
        """Test research agent with mocked OpenRouter"""
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Research results"))]
        mock_client.return_value.chat.completions.create.return_value = mock_response

        agent = ResearchAgent()
        result = agent.gather_information("quantum computing")

        self.assertIsNotNone(result)
        self.assertEqual(result, "Research results")
        ]]>
      </test-example>
    </unit-testing>

    <integration-testing>
      <technical-explanation>
        Integration tests that validate actual OpenRouter API connectivity and
        response handling for end-to-end functionality verification.
      </technical-explanation>
      <simple-explanation>
        Tests that actually connect to OpenRouter to make sure your setup works
        correctly with the real API service.
      </simple-explanation>
      <test-example>
        <![CDATA[
def test_openrouter_connection():
    """Test actual OpenRouter API connection"""
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )

    try:
        response = client.chat.completions.create(
            model="openai/gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Say 'test passed'"}],
            max_tokens=10
        )
        assert "test passed" in response.choices[0].message.content.lower()
        print("âœ“ OpenRouter connection successful")
        return True
    except Exception as e:
        print(f"âœ— OpenRouter connection failed: {e}")
        return False
        ]]>
      </test-example>
    </integration-testing>
  </testing-integration>

  <migration-strategies>
    <migration-comparison>
      <before-state title="Direct Anthropic API">
        <technical-explanation>Direct provider integration requiring provider-specific SDK and response handling</technical-explanation>
        <simple-explanation>Old way: separate code for each AI company's API</simple-explanation>
        <code-example>
          <![CDATA[
import anthropic

client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
response = client.messages.create(
    model="claude-3-opus-20240229",
    messages=[{"role": "user", "content": "Hello"}]
)
          ]]>
        </code-example>
      </before-state>

      <after-state title="OpenRouter Unified">
        <technical-explanation>Unified integration using OpenAI SDK interface for all providers</technical-explanation>
        <simple-explanation>New way: one interface works with all AI companies</simple-explanation>
        <code-example>
          <![CDATA[
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)
response = client.chat.completions.create(
    model="anthropic/claude-3-opus",
    messages=[{"role": "user", "content": "Hello"}]
)
          ]]>
        </code-example>
      </after-state>
    </migration-comparison>

    <migration-benefits>
      <benefit>Reduced code complexity and maintenance overhead</benefit>
      <benefit>Unified error handling and retry logic</benefit>
      <benefit>Simplified dependency management</benefit>
      <benefit>Automatic failover capabilities</benefit>
      <benefit>Cost optimization through smart routing</benefit>
    </migration-benefits>
  </migration-strategies>

  <performance-monitoring>
    <monitoring-implementation>
      <technical-explanation>
        Performance tracking system monitoring latency, throughput, and token efficiency
        across different models and usage patterns for optimization insights.
      </technical-explanation>
      <simple-explanation>
        System that measures how fast AI responses come back and how efficiently you're
        using different models so you can optimize performance.
      </simple-explanation>
      <implementation>
        <![CDATA[
class PerformanceMonitor:
    def __init__(self):
        self.metrics = []

    def log_request(self, start_time, end_time, model, tokens):
        """Log performance metrics"""
        latency = end_time - start_time
        self.metrics.append({
            "timestamp": start_time,
            "latency": latency,
            "model": model,
            "tokens": tokens,
            "tokens_per_second": tokens / latency if latency > 0 else 0
        })

    def get_statistics(self):
        """Calculate performance statistics"""
        if not self.metrics:
            return {}

        latencies = [m["latency"] for m in self.metrics]
        return {
            "avg_latency": sum(latencies) / len(latencies),
            "p95_latency": sorted(latencies)[int(len(latencies) * 0.95)],
            "total_requests": len(self.metrics),
            "total_tokens": sum(m["tokens"] for m in self.metrics)
        }
        ]]>
      </implementation>
      <tracked-metrics>
        <metric>Request latency (average and 95th percentile)</metric>
        <metric>Tokens per second throughput</metric>
        <metric>Total requests and token usage</metric>
        <metric>Model performance comparison</metric>
      </tracked-metrics>
    </monitoring-implementation>
  </performance-monitoring>

  <security-best-practices>
    <practice-list>
      <practice>Never hardcode API keys in source code</practice>
      <practice>Use environment variables for all credentials</practice>
      <practice>Implement request signing when available</practice>
      <practice>Monitor usage patterns for anomalies</practice>
      <practice>Set spending limits and budget alerts</practice>
      <practice>Use separate keys for development and production</practice>
      <practice>Rotate API keys on regular schedule</practice>
      <practice>Implement application-level rate limiting</practice>
    </practice-list>
    <security-rationale>
      <technical-explanation>
        Security framework preventing credential exposure, unauthorized access,
        and cost overruns through defense-in-depth approach.
      </technical-explanation>
      <simple-explanation>
        Multiple layers of protection to keep your API keys safe, prevent unauthorized
        use, and avoid surprise bills from excessive usage.
      </simple-explanation>
    </security-rationale>
  </security-best-practices>

  <key-takeaways>
    <technical>
      <takeaway>OpenRouter provides unified interface reducing integration complexity by 75%</takeaway>
      <takeaway>Dynamic model selection enables cost optimization while maintaining quality standards</takeaway>
      <takeaway>Built-in failover and retry mechanisms improve production reliability significantly</takeaway>
      <takeaway>Performance monitoring enables data-driven optimization of model selection strategies</takeaway>
    </technical>
    <simple>
      <takeaway>One simple setup connects you to 400+ AI models instead of complex individual integrations</takeaway>
      <takeaway>Smart model selection saves money by using the right tool for each job</takeaway>
      <takeaway>Automatic backup switching keeps your app working even when AI services have problems</takeaway>
      <takeaway>Performance tracking helps you optimize which models to use for best results</takeaway>
    </simple>
  </key-takeaways>

  <cross-references>
    <reference type="overview" target="25_openrouter_overview.xml"/>
    <reference type="model-routing" target="27_openrouter_model_routing.xml"/>
    <reference type="cost-optimization" target="28_openrouter_cost_optimization.xml"/>
    <reference type="production-patterns" target="29_openrouter_production_patterns.xml"/>
    <reference type="level-4-architecture" target="../foundation/architecture-phases.xml"/>
  </cross-references>
</reference>
