<?xml version="1.0" encoding="UTF-8"?>
<document type="reference" domain="quality" version="1.0.0" xmlns="https://ai-podcasts-nobody-knows.com/claude-docs">

  <metadata>
    <title>Hallucination Prevention Guide</title>
    <phase>walk</phase>
    <skill-level>intermediate</skill-level>
    <claude-optimization>true</claude-optimization>
    <learning-outcomes>
      <outcome>Master anti-hallucination techniques and validation workflows</outcome>
      <outcome>Implement multi-source fact verification systems</outcome>
      <outcome>Develop uncertainty acknowledgment practices</outcome>
      <outcome>Build automated validation pipelines</outcome>
    </learning-outcomes>
    <prerequisites>
      <prerequisite>Understanding of AI limitations and bias</prerequisite>
      <prerequisite>Basic research and fact-checking skills</prerequisite>
    </prerequisites>
    <estimated-time>45 minutes</estimated-time>
  </metadata>

  <content>
    <section type="introduction" id="anti-hallucination-policy">
    </section>

    <section type="implementation" id="prevention-strategies">

      <examples>
        <example type="basic">
          <scenario>RAG Implementation for API Documentation</scenario>
          <implementation>Search authoritative sources first → Extract relevant information → Generate based on retrieved facts → Cite sources in output</implementation>
          <explanation>Like doing homework by reading the textbook first instead of guessing - much more accurate results</explanation>
        </example>
        <example type="intermediate">
          <scenario>Uncertainty Acknowledgment for Performance Claims</scenario>
          <implementation>If confidence &lt; 80%, use phrases like "I'm not certain" or "I need to verify this"</implementation>
          <explanation>Like a good teacher saying "Let me look that up" instead of guessing wrong information</explanation>
        </example>
        <example type="advanced">
          <scenario>Direct Quote Grounding for Technical Specifications</scenario>
          <implementation>Find word-for-word quotes supporting claims → If no quote exists, retract the claim → Build responses from verified quotes</implementation>
          <explanation>Like being a detective who only draws conclusions from actual evidence you can point to</explanation>
        </example>
      </examples>
    </section>

    <section type="implementation" id="validation-workflow">

      <instructions>
        <step number="1" validation-command="grep -c 'factual.*claim' claims_analysis.txt">Identify all statements that assert facts (statistics, events, specifications)</step>
        <step number="2" validation-command="wc -l search_results.txt | awk '$1 >= 3 {print &quot;Minimum research completed&quot;}'">Conduct 3-10 searches with adaptive rules for conflicting information</step>
        <step number="3" validation-command="python3 validate_cross_references.py sources.json">Compare information across sources using decision tree logic</step>
        <step number="4" validation-command="grep -E 'Source:.*URL.*Date:' final_content.md">Ensure every factual claim includes source URL and access date</step>
        <step number="5" validation-command="grep -E 'confidence.*[0-9]+%' confidence_disclosure.txt">Disclose confidence levels using standard ranges (90-100%, 70-89%, 50-69%, &lt;50%)</step>
      </instructions>

      <validation>
        <validation-command>python3 check_validation_completeness.py content.md</validation-command>
        <validation-command>grep -c "confidence.*uncertain" content.md</validation-command>
        <success-criteria>All factual claims have source citations</success-criteria>
        <success-criteria>Confidence levels disclosed for uncertain information</success-criteria>
        <success-criteria>Minimum 3 sources for technical specifications</success-criteria>
        <success-criteria>Uncertainty acknowledged when confidence &lt; 80%</success-criteria>
      </validation>
    </section>

    <section type="reference" id="research-categories">

      <examples>
        <example type="basic">
          <scenario>Technical Specifications Validation</scenario>
          <implementation>Official documentation + GitHub repos + technical papers, within last 2 years</implementation>
          <explanation>Technology changes fast, so you need current sources from the people who built it</explanation>
        </example>
        <example type="intermediate">
          <scenario>Statistics and Data Validation</scenario>
          <implementation>Find original study + one additional primary source, cite methodology</implementation>
          <explanation>Numbers can be misleading, so you need to see how they were calculated originally</explanation>
        </example>
      </examples>
    </section>

    <section type="troubleshooting" id="hallucination-red-flags">

      <examples>
        <example type="anti-pattern">
          <scenario>Red Flag: Specific Numbers Without Source</scenario>
          <implementation>WRONG: "increases performance by 40%" - RIGHT: "increases performance by 40% (Source: benchmark study XYZ, 2025)"</implementation>
          <explanation>Specific numbers sound authoritative but are often the most likely to be hallucinated</explanation>
        </example>
        <example type="anti-pattern">
          <scenario>Red Flag: Absolute Statements</scenario>
          <implementation>WRONG: "always works" - RIGHT: "typically works in most cases, with some exceptions"</implementation>
          <explanation>Real world rarely has absolutes - adding nuance makes claims more accurate</explanation>
        </example>
      </examples>
    </section>

    <section type="advanced" id="claude-code-automation">

      <examples>
        <example type="advanced">
          <scenario>Automated Thinking Mode Escalation</scenario>
          <implementation>If confidence &lt; 60% → ultrathink mode, if safety-critical → maximum validation with expert sources</implementation>
          <explanation>Like automatically calling in experts when dealing with difficult or dangerous situations</explanation>
        </example>
        <example type="advanced">
          <scenario>Parallel Fact-Checking Subagents</scenario>
          <implementation>Multiple specialized agents verify different aspects simultaneously: authority, recency, bias, cross-references</implementation>
          <explanation>Like having a team of specialists each check different parts of the information at the same time</explanation>
        </example>
      </examples>
    </section>

    <section type="reference" id="uncertainty-language">

      <examples>
        <example type="basic">
          <scenario>High Uncertainty Expression</scenario>
          <implementation>"I don't have reliable information about this specific claim - let me research it properly first"</implementation>
          <explanation>Better to admit you don't know than to guess and be wrong</explanation>
        </example>
        <example type="basic">
          <scenario>Qualified Statement with Source</scenario>
          <implementation>"According to the 2025 FastAPI documentation, the default timeout is 30 seconds"</implementation>
          <explanation>Shows exactly where the information comes from and when it was accurate</explanation>
        </example>
      </examples>
    </section>
  </content>

  <cross-references>
    <reference file="01_change_approval_requirements.xml" section="validation" type="prerequisite">Change approval workflow validation requirements</reference>
    <reference file="03_tdd_requirements_specification.xml" section="behavioral-testing" type="related">Behavioral testing approaches for AI validation</reference>
    <reference file="../operations/01_troubleshooting_guide.xml" section="debugging" type="related">Systematic debugging and problem resolution</reference>
    <reference file="../claude-code/21_mcp_integration_guide.xml" section="web-search" type="advanced">Web search MCP integration for automated fact-checking</reference>
    <reference file="../claude-code/19_thinking_modes_guide.xml" section="escalation" type="advanced">Thinking mode escalation for complex validation</reference>
  </cross-references>

</document>
