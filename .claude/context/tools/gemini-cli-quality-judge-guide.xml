<!-- markdownlint-disable-file -->
<?xml version="1.0" encoding="UTF-8"?>
<document type="tool-guide" version="1.0.0">
  <metadata>
    <title>Gemini CLI Quality Judge - Comprehensive Implementation Guide</title>
    <created>2025-01-14</created>
    <author>Claude Code AI Assistant</author>
    <purpose>Complete guide for using Gemini CLI as an LLM-as-Judge quality evaluator</purpose>
    <cost-model>Gemini 2.5 Flash: $0.15/M input, $0.60/M output tokens</cost-model>
  </metadata>

  <educational-context>
    <technical-explanation>
      Gemini CLI serves as an independent LLM judge implementing pointwise scoring with structured JSON output.
      This approach leverages Gemini 2.5 Flash's large context window (1M tokens) and cost efficiency for
      scalable quality evaluation. The system uses Likert scale scoring (1-5) with clear rubrics to ensure
      consistent, measurable quality assessment aligned with industry best practices for LLM evaluation.
    </technical-explanation>
    
    <simple-explanation>
      Think of Gemini CLI as an impartial judge at a competition. Just like having multiple judges reduces bias
      in competitions, using Gemini alongside Claude gives us two different perspectives on quality. The CLI
      works like a scoring sheet where Gemini rates different aspects from 1-5, then we average the scores
      to get a final grade. It's fast, cheap, and gives consistent results.
    </simple-explanation>
    
    <learning-value>
      This teaches you enterprise-grade quality assurance patterns, multi-model consensus validation,
      cost-optimized AI operations, and production-ready error handling strategies that apply to any
      AI-powered evaluation system.
    </learning-value>
  </educational-context>

  <installation-verification>
    <command>gemini --version</command>
    <expected-output>0.1.21 or higher</expected-output>
    <install-if-missing>npm install -g @google/gemini-cli</install-if-missing>
    <free-tier-limits>
      <requests-per-minute>60</requests-per-minute>
      <requests-per-day>1000</requests-per-day>
      <context-window>1048576 tokens input</context-window>
    </free-tier-limits>
  </installation-verification>

  <core-usage-patterns>
    <pattern name="Basic Non-Interactive Evaluation">
      <description>Simple one-shot evaluation with prompt</description>
      <command>gemini -p "Evaluate this text: [content]"</command>
      <use-case>Quick quality checks, simple scoring</use-case>
    </pattern>
    
    <pattern name="File-Based Evaluation">
      <description>Evaluate content from file with structured prompt</description>
      <command>cat script.md | gemini -p "$(cat evaluation-prompt.txt)"</command>
      <use-case>Production podcast script evaluation</use-case>
    </pattern>
    
    <pattern name="JSON Output Enforcement">
      <description>Ensure JSON-only output for parsing</description>
      <command>gemini -p "Output ONLY valid JSON: {evaluation criteria}"</command>
      <use-case>Automated pipeline integration</use-case>
    </pattern>
    
    <pattern name="Template Variable Substitution">
      <description>Dynamic prompt generation with bash variables</description>
      <command>
        SCRIPT="$(&lt;script.md)"
        PROMPT="Evaluate with complexity level $LEVEL: $SCRIPT"
        echo "$PROMPT" | gemini -p -
      </command>
      <use-case>Adaptive evaluation based on episode complexity</use-case>
    </pattern>
  </core-usage-patterns>

  <evaluation-rubric>
    <scoring-scale>
      <range>1-5 Likert Scale</range>
      <interpretation>
        <score value="1">Poor - Critical issues, needs complete revision</score>
        <score value="2">Below Average - Multiple significant issues</score>
        <score value="3">Average - Acceptable with some improvements needed</score>
        <score value="4">Good - Minor improvements possible</score>
        <score value="5">Excellent - Meets or exceeds all criteria</score>
      </interpretation>
    </scoring-scale>
    
    <evaluation-criteria>
      <criterion name="Factual Accuracy" weight="0.25">
        <description>Verification of claims, statistics, and technical details</description>
        <rubric>
          <score-5>All facts verified correct, sources credible, no errors</score-5>
          <score-4>One minor factual issue that doesn't affect main points</score-4>
          <score-3>2-3 minor factual issues or one moderate issue</score-3>
          <score-2>Multiple factual errors affecting understanding</score-2>
          <score-1>Significant misinformation or unverified claims</score-1>
        </rubric>
      </criterion>
      
      <criterion name="Audience Comprehension" weight="0.25">
        <description>Clarity and accessibility for general audience</description>
        <rubric>
          <score-5>Crystal clear, perfect progression, accessible to all</score-5>
          <score-4>Mostly clear with one or two complex sections</score-4>
          <score-3>Generally understandable with some jargon</score-3>
          <score-2>Often confusing, too much unexplained terminology</score-2>
          <score-1>Incomprehensible to general audience</score-1>
        </rubric>
      </criterion>
      
      <criterion name="Brand Alignment" weight="0.30">
        <description>Intellectual humility and questioning tone</description>
        <rubric>
          <score-5>Perfect brand voice, 5+ humility phrases per 1000 words</score-5>
          <score-4>Strong brand alignment, 3-4 humility phrases</score-4>
          <score-3>Adequate brand voice, 2-3 humility phrases</score-3>
          <score-2>Weak brand alignment, 1-2 humility phrases</score-2>
          <score-1>No brand alignment, absolutist tone</score-1>
        </rubric>
      </criterion>
      
      <criterion name="Engagement Quality" weight="0.20">
        <description>Narrative flow and listener retention</description>
        <rubric>
          <score-5>Captivating throughout, excellent pacing</score-5>
          <score-4>Mostly engaging with minor slow points</score-4>
          <score-3>Generally interesting with some flat sections</score-3>
          <score-2>Often boring or poorly paced</score-2>
          <score-1>Consistently unengaging</score-1>
        </rubric>
      </criterion>
    </evaluation-criteria>
  </evaluation-rubric>

  <json-output-schema>
    <schema>
{
  "evaluation_id": "string (UUID)",
  "timestamp": "string (ISO 8601)",
  "model": "gemini-2.5-flash",
  "scores": {
    "factual_accuracy": "integer (1-5)",
    "audience_comprehension": "integer (1-5)",
    "brand_alignment": "integer (1-5)",
    "engagement_quality": "integer (1-5)"
  },
  "weighted_average": "float (1.0-5.0)",
  "pass_threshold": 3.5,
  "pass_fail": "PASS|FAIL",
  "critical_issues": [
    {
      "category": "string",
      "description": "string",
      "severity": "HIGH|MEDIUM|LOW"
    }
  ],
  "improvements": [
    {
      "priority": "integer (1-3)",
      "suggestion": "string"
    }
  ],
  "strengths": ["string"],
  "metrics": {
    "word_count": "integer",
    "humility_phrases": "integer",
    "questions_count": "integer",
    "estimated_duration": "float (minutes)"
  },
  "cost_estimate": "string ($0.0000)"
}
    </schema>
  </json-output-schema>

  <implementation-commands>
    <command-sequence>
      <step number="1" name="Prepare Script">
        <bash>
# Save script to temporary file
TIMESTAMP=$(date +%s)
SCRIPT_FILE="/tmp/podcast_script_${TIMESTAMP}.md"
cat script.md > "$SCRIPT_FILE"
        </bash>
      </step>
      
      <step number="2" name="Create Evaluation Prompt">
        <bash>
# Generate evaluation prompt with rubric
EVAL_PROMPT="You are a podcast quality evaluator. Evaluate the following script and output ONLY valid JSON.

SCORING RUBRIC (1-5 scale):
1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent

Evaluate these criteria:
- Factual Accuracy: Verify all claims and technical details
- Audience Comprehension: Assess clarity for general audience
- Brand Alignment: Count intellectual humility phrases (target: 5 per 1000 words)
- Engagement Quality: Evaluate narrative flow and pacing

Output this exact JSON structure with your scores:
{json_schema}

Script to evaluate:
$(cat $SCRIPT_FILE)"
        </bash>
      </step>
      
      <step number="3" name="Execute Gemini Evaluation">
        <bash>
# Run Gemini CLI with timeout and error handling
OUTPUT_FILE="/tmp/gemini_result_${TIMESTAMP}.json"
ERROR_FILE="/tmp/gemini_error_${TIMESTAMP}.log"

timeout 30 bash -c "echo '$EVAL_PROMPT' | gemini -p - 2>'$ERROR_FILE'" > "$OUTPUT_FILE"

# Check exit code
if [ $? -ne 0 ]; then
    echo "Error: Gemini evaluation failed"
    cat "$ERROR_FILE"
    exit 1
fi
        </bash>
      </step>
      
      <step number="4" name="Validate JSON Output">
        <bash>
# Validate JSON structure
if ! jq empty "$OUTPUT_FILE" 2>/dev/null; then
    echo "Error: Invalid JSON response"
    
    # Retry with simplified prompt
    echo "Retrying with simplified prompt..."
    gemini -p "Rate this podcast script 1-5 on: factual accuracy, comprehension, brand alignment, engagement. Output only JSON with these scores." < "$SCRIPT_FILE" > "$OUTPUT_FILE"
fi

# Extract and display results
if jq empty "$OUTPUT_FILE" 2>/dev/null; then
    WEIGHTED_AVG=$(jq -r '.weighted_average' "$OUTPUT_FILE")
    PASS_FAIL=$(jq -r '.pass_fail' "$OUTPUT_FILE")
    echo "Evaluation complete: Score=$WEIGHTED_AVG, Result=$PASS_FAIL"
else
    echo "Error: Could not parse Gemini response"
    exit 1
fi
        </bash>
      </step>
      
      <step number="5" name="Cost Tracking">
        <bash>
# Estimate token usage and cost
WORD_COUNT=$(wc -w < "$SCRIPT_FILE")
INPUT_TOKENS=$((WORD_COUNT * 4 / 3))  # Approximate tokens
OUTPUT_TOKENS=500  # Typical JSON response

# Calculate cost (Gemini 2.5 Flash pricing)
INPUT_COST=$(echo "scale=6; $INPUT_TOKENS * 0.15 / 1000000" | bc)
OUTPUT_COST=$(echo "scale=6; $OUTPUT_TOKENS * 0.60 / 1000000" | bc)
TOTAL_COST=$(echo "scale=6; $INPUT_COST + $OUTPUT_COST" | bc)

echo "Evaluation cost: \$$TOTAL_COST (Input: $INPUT_TOKENS tokens, Output: $OUTPUT_TOKENS tokens)"

# Log to cost tracking file
echo "$(date -Iseconds),gemini_evaluation,$TOTAL_COST" >> .claude/logs/api_costs.csv
        </bash>
      </step>
    </command-sequence>
  </implementation-commands>

  <error-handling>
    <scenario name="Gemini CLI Not Found">
      <detection>command -v gemini returns non-zero</detection>
      <resolution>
        <step>Check npm global bin in PATH: echo $PATH | grep npm</step>
        <step>Reinstall: npm install -g @google/gemini-cli</step>
        <step>Verify: which gemini</step>
      </resolution>
    </scenario>
    
    <scenario name="Rate Limit Exceeded">
      <detection>Error message contains "rate limit" or "quota"</detection>
      <resolution>
        <step>Implement exponential backoff: sleep $((2**retry_count))</step>
        <step>Track daily usage: Check request count log</step>
        <step>Consider upgrading to paid tier if consistent issue</step>
      </resolution>
    </scenario>
    
    <scenario name="Invalid JSON Response">
      <detection>jq validation fails</detection>
      <resolution>
        <step>Retry with explicit JSON instruction in prompt</step>
        <step>Use simplified prompt template</step>
        <step>Extract scores using regex as fallback</step>
      </resolution>
    </scenario>
    
    <scenario name="Timeout">
      <detection>Command exceeds 30-second limit</detection>
      <resolution>
        <step>Reduce script size (chunk if needed)</step>
        <step>Simplify evaluation criteria</step>
        <step>Check network connectivity</step>
      </resolution>
    </scenario>
  </error-handling>

  <optimization-strategies>
    <strategy name="Prompt Caching">
      <description>Reuse evaluation prompts to leverage Gemini's context caching</description>
      <implementation>Store prompt template, only vary script content</implementation>
      <savings>Up to 42% cost reduction on repeated evaluations</savings>
    </strategy>
    
    <strategy name="Batch Processing">
      <description>Evaluate multiple scripts in sequence to optimize API usage</description>
      <implementation>Queue evaluations, process in 5-minute windows</implementation>
      <savings>Better rate limit utilization, reduced overhead</savings>
    </strategy>
    
    <strategy name="Selective Evaluation">
      <description>Only evaluate scripts that pass basic automated checks</description>
      <implementation>Pre-filter with word count, structure validation</implementation>
      <savings>30-40% reduction in API calls</savings>
    </strategy>
    
    <strategy name="Response Minimization">
      <description>Request only essential fields in JSON output</description>
      <implementation>Specify exact JSON structure, avoid verbose explanations</implementation>
      <savings>20-30% reduction in output tokens</savings>
    </strategy>
  </optimization-strategies>

  <integration-points>
    <point name="Episode Production Pipeline">
      <location>After 03_script_writer, parallel with 04_quality_claude</location>
      <trigger>Script completion signal</trigger>
      <output>Evaluation JSON to 06_feedback_synthesizer</output>
    </point>
    
    <point name="Quality Gate Enforcement">
      <location>projects/nobody-knows/config/quality_gates.json</location>
      <action>Compare scores against thresholds</action>
      <decision>PASS → Continue, FAIL → Retry with feedback</decision>
    </point>
    
    <point name="Cost Tracking">
      <location>.claude/logs/api_costs.csv</location>
      <format>timestamp,service,cost</format>
      <monitoring>Daily aggregation, budget alerts</monitoring>
    </point>
  </integration-points>

  <testing-checklist>
    <test name="Installation Verification">
      <command>gemini --version</command>
      <expected>Version number displayed</expected>
    </test>
    
    <test name="Basic Evaluation">
      <command>echo "Test podcast script about AI." | gemini -p "Rate 1-5"</command>
      <expected>Response with rating</expected>
    </test>
    
    <test name="JSON Output">
      <command>gemini -p "Output only JSON: {\"test\": true}"</command>
      <expected>Valid JSON response</expected>
    </test>
    
    <test name="File Processing">
      <command>cat sample_script.md | gemini -p "Evaluate"</command>
      <expected>Complete evaluation response</expected>
    </test>
    
    <test name="Error Handling">
      <command>timeout 1 gemini -p "Long evaluation task"</command>
      <expected>Graceful timeout handling</expected>
    </test>
  </testing-checklist>

  <best-practices>
    <practice>Always validate JSON output before parsing</practice>
    <practice>Implement retry logic with exponential backoff</practice>
    <practice>Track costs per evaluation for budget management</practice>
    <practice>Use specific version in prompts for consistency</practice>
    <practice>Cache evaluation results to avoid duplicate API calls</practice>
    <practice>Monitor rate limits and implement queuing if needed</practice>
    <practice>Test prompts with various script types before production</practice>
    <practice>Document any prompt changes for reproducibility</practice>
  </best-practices>

  <troubleshooting-faq>
    <question>
      <q>Gemini returns narrative text instead of JSON</q>
      <a>Add "Output ONLY valid JSON, no other text:" to prompt start</a>
    </question>
    
    <question>
      <q>Evaluation scores seem inconsistent</q>
      <a>Include specific rubric definitions in every prompt</a>
    </question>
    
    <question>
      <q>Command hangs without response</q>
      <a>Check internet connection, verify API availability, add timeout wrapper</a>
    </question>
    
    <question>
      <q>Cost higher than expected</q>
      <a>Review token counting, implement response size limits, use Flash model</a>
    </question>
  </troubleshooting-faq>

  <conclusion>
    <summary>
      Gemini CLI provides a cost-effective, reliable quality evaluation system when properly configured.
      With structured prompts, JSON validation, and robust error handling, it serves as an excellent
      complement to Claude-based evaluation, reducing bias and improving overall quality assurance.
    </summary>
    
    <next-steps>
      <step>Test basic Gemini CLI functionality</step>
      <step>Validate JSON output with sample scripts</step>
      <step>Integrate into production pipeline</step>
      <step>Monitor costs and performance</step>
      <step>Iterate on prompts based on results</step>
    </next-steps>
  </conclusion>
</document>