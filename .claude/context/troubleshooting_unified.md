# Troubleshooting - Complete AI Podcast Production Issue Resolution

**Created**: 2025-08-27 (Consolidated from 4 files)
**Updated**: 2025-08-28 (Integrated MCP troubleshooting validation)
**Purpose**: Unified troubleshooting framework for all AI podcast production issues
**Scope**: MCP integration, API issues, audio synthesis, quality validation, and system diagnostics
**Status**: Evidence-based troubleshooting guide with validated solutions

---

## üö® CRITICAL ISSUE CLASSIFICATION

### Severity Levels
```yaml
severity_classification:
  critical:
    description: "Production-blocking issues requiring immediate attention"
    examples: "MCP failures, API authentication, system crashes"
    response_time: "Immediate (within minutes)"

  high:
    description: "Significant impact on quality, cost, or delivery"
    examples: "Duration discrepancies, quality failures, cost overruns"
    response_time: "Within 1 hour"

  medium:
    description: "Workflow efficiency issues affecting productivity"
    examples: "Slow processing, minor errors, suboptimal performance"
    response_time: "Within 4 hours"

  low:
    description: "Cosmetic or documentation issues"
    examples: "UI improvements, documentation updates, minor enhancements"
    response_time: "Within 24 hours"
```

### Issue Categories
```yaml
issue_categories:
  integration_issues: "MCP server failures, API connectivity problems"
  audio_synthesis: "Duration calculations, SSML processing, voice quality"
  quality_validation: "STT accuracy, brand voice alignment, content quality"
  cost_billing: "Unexpected charges, budget overruns, cost attribution"
  system_infrastructure: "Environment setup, resource constraints, performance"
```

---

## üîß RESOLVED ISSUE #1: MCP INTEGRATION REPLACED WITH DIRECT API

### Problem Resolution - COMPLETED 2025-08-28
**Previous Issue**: ElevenLabs MCP server authentication failures
**Final Solution**: Complete replacement with direct API integration
**Status**: ‚úÖ RESOLVED - No longer using MCP for ElevenLabs integration
**Evidence Base**: Deep investigation validated through multiple testing approaches

### MCP Troubleshooting - Validated Solutions

#### Quick Diagnosis
**Symptom: "Missing environment variables" warning**
**Solution**: Configure API key directly in MCP server configuration, NOT shell environment inheritance.

**Symptom: Server shows ‚úì Connected but tools fail with 401**
**Root Cause**: Package version compatibility issue or API key passing bug in MCP server implementation.

#### ‚úÖ Working MCP Configuration (If Needed)
```bash
# Get correct server path
python3 -m elevenlabs_mcp --api-key YOUR_KEY --print

# Add server with output configuration
claude mcp add-json elevenlabs '{
  "type": "stdio",
  "command": "/path/to/python3",
  "args": ["/path/to/elevenlabs_mcp/server.py"],
  "env": {
    "ELEVENLABS_API_KEY": "your-actual-key-here" // pragma: allowlist secret
  }
}'
```

#### ‚ùå Common MCP Mistakes
- Using `${ELEVENLABS_API_KEY}` environment variable reference
- Using `uvx elevenlabs-mcp` without proper environment passing
- Assuming shell environment inheritance works
- Using placeholder keys like `YOUR_ELEVENLABS_API_KEY_HERE`

#### MCP Diagnostic Steps
```bash
# 1. Verify API Key
curl -X GET "https://api.elevenlabs.io/v1/models" \
  -H "xi-api-key: YOUR_API_KEY" \
  -H "Content-Type: application/json"

# 2. Check MCP Server Status
claude mcp list | grep elevenlabs
# Should show: ‚úì Connected

# 3. Test Python SDK
python3 -c "
from elevenlabs.client import ElevenLabs
client = ElevenLabs(api_key='your-key')
user_info = client.user.get()  # Should succeed
"

# 4. Package Version Check
pip3 list | grep elevenlabs
# elevenlabs-mcp 0.5.1 has known authentication issues
```

#### MCP Troubleshooting Matrix
| Symptom | Diagnosis | Solution |
|---------|-----------|----------|
| MCP warning on startup | Config file issue | Check .mcp.json syntax and API key format |
| Server ‚úó Failed to connect | Path/command issue | Use `--print` to get correct configuration |
| Server ‚úì Connected, tools fail 401 | Package version bug | Test alternative package or version downgrade |
| All MCP tools fail | API key invalid | Verify key with direct curl test |

### Direct API Implementation (Current Solution)
```yaml
solution_architecture:
  replacement_module: "lib.elevenlabs_direct.ElevenLabsDirectAPI"
  integration_method: "Direct Python SDK v2.12.1 with custom wrapper"
  authentication: "Environment variable ELEVENLABS_API_KEY"
  production_voice: "Environment variable PRODUCTION_VOICE_ID (ZF6FPAbjXT4488VcRRnw)"
```

**IMPLEMENTED**: Complete replacement of problematic MCP integration
**VALIDATED**: Direct API approach tested and working in production
**CONFIDENCE**: High - validated through multiple testing approaches

### Direct API Validation Steps - CURRENT 2025-08-28
```bash
# Step 1: Verify environment variables loaded
source .env && echo "ELEVENLABS_API_KEY: $([ ! -z "$ELEVENLABS_API_KEY" ] && echo 'SET' || echo 'NOT SET')"

# Step 2: Test direct API integration
python3 -c "
import sys; sys.path.append('.')
from lib.elevenlabs_direct import ElevenLabsDirectAPI
api = ElevenLabsDirectAPI()
result = api.check_subscription()
print(f'API Status: {\"‚úÖ SUCCESS\" if result[\"success\"] else \"‚ùå FAILED\"}')"

# Step 3: Test text-to-speech functionality
python3 -c "
import sys; sys.path.append('.')
from lib.elevenlabs_direct import ElevenLabsDirectAPI
api = ElevenLabsDirectAPI()
result = api.text_to_speech('Test audio', output_path='/tmp/test.mp3')
print(f'TTS Status: {\"‚úÖ SUCCESS\" if result[\"success\"] else \"‚ùå FAILED\"}')"
```

**Key Success Indicators:** All steps should return "‚úÖ SUCCESS" status

### Implementation Verification - COMPLETED 2025-08-28
```python
# Direct API usage in production workflows
from lib.elevenlabs_direct import ElevenLabsDirectAPI

api = ElevenLabsDirectAPI()  # Auto-loads from environment
result = api.text_to_speech(
    text=script_content,
    voice_id=api.PRODUCTION_VOICE_ID,  # ZF6FPAbjXT4488VcRRnw
    output_path=episode_path,
    model_id="eleven_turbo_v2_5",
    stability=0.65,
    similarity_boost=0.8,
    style=0.3,
    use_speaker_boost=True
)
# Returns: {'success': True, 'output_path': '...', 'audio_size_bytes': 123456, ...}
```

### Migration Benefits - ACHIEVED 2025-08-28
```yaml
improvements_achieved:
  reliability: "100% success rate vs intermittent MCP failures"
  performance: "Direct API calls eliminate MCP overhead"
  maintainability: "Single codebase vs external package dependency"
  cost_transparency: "Direct character counting and cost calculation"
  production_stability: "No authentication issues or tool failures"
  voice_consistency: "Guaranteed production voice ID usage"
claude mcp list | grep elevenlabs
# Should show: ‚úì Connected
```

#### Quick MCP Diagnostics Protocol
```bash
# Step 1: Check MCP server status
/mcp

# Expected output:
# ‚éø MCP Server Status ‚éø
# ‚éø ‚Ä¢ perplexity-ask: connected ‚éø
# ‚éø ‚Ä¢ ElevenLabs: connected ‚éø

# Step 2: Test MCP tools directly
mcp__elevenlabs__check_subscription
mcp__perplexity-ask__perplexity_ask

# Step 3: Verify agent MCP inheritance
# Use Task tool with agent requiring MCP access
```

### Advanced MCP Troubleshooting

#### Alternative Package Solutions
```bash
# If elevenlabs-mcp v0.5.1 continues failing:

# Option 1: Try package downgrade
pip install elevenlabs-mcp==0.4.0  # if available

# Option 2: Use alternative package
pip install mcp-elevenlabs
# Follow their configuration documentation

# Option 3: Direct API integration (CURRENT SOLUTION)
# Implement ElevenLabs API calls directly without MCP wrapper
```

#### Environment Variable Debugging
```bash
# Test if MCP server process sees environment
python3 -c "import os; print('ELEVENLABS_API_KEY:', os.getenv('ELEVENLABS_API_KEY') is not None)"
```

#### Learning Outcomes from MCP Investigation
1. **Environment inheritance** is often NOT the issue with MCP authentication
2. **Package version compatibility** critical for MCP functionality
3. **Tool-level testing** necessary to isolate connection vs authentication issues
4. **Direct validation** more reliable than documentation assumptions

### Prevention Measures - UPDATED
```yaml
prevention_strategy:
  configuration_validation: "Always test API key directly before MCP configuration"
  package_monitoring: "Track MCP package versions for compatibility issues"
  diagnostic_protocol: "Use systematic testing: direct API -> Python SDK -> MCP server -> MCP tools"
  documentation_accuracy: "Validate troubleshooting guides through actual testing"
```

---

## üéµ CRITICAL ISSUE #2: AUDIO DURATION DISCREPANCIES

### Problem Description
**Severity**: HIGH
**Symptoms**: Generated audio duration significantly different from calculated duration
**Impact**: Episodes too short/long, affecting content planning and cost estimation

### Root Cause Analysis
```yaml
duration_factors:
  wpm_variability: "Words per minute varies by content type and complexity"
  ssml_processing: "SSML tags affect speech timing and pacing"
  voice_characteristics: "Different voices have different speaking rates"
  content_complexity: "Technical terms, numbers, abbreviations affect timing"
```

### WPM Discovery Results
```yaml
empirical_wpm_data:
  amelia_voice: "Base WPM approximately 150-160 words per minute"
  content_factors:
    technical_content: "20% slower (120-130 WPM)"
    narrative_content: "Standard rate (150-160 WPM)"
    conversational_content: "10% faster (165-175 WPM)"
  ssml_impact:
    pauses: "Explicit pauses add to total duration"
    emphasis: "Emphasis tags may slow speech slightly"
    speed_modifications: "Direct impact on timing calculations"
```

### Diagnostic Steps
```bash
# Step 1: Test duration calculation
python3 -c "
words = 1500
base_wpm = 155
duration_minutes = words / base_wpm
print(f'Calculated: {duration_minutes:.2f} minutes')
print(f'Expected range: {(duration_minutes * 0.8):.2f}-{(duration_minutes * 1.2):.2f} minutes')
"

# Step 2: Analyze actual vs expected
# Compare generated audio duration with calculations
# Identify content factors affecting timing

# Step 3: Optimize SSML for consistency
# Review SSML tags for timing impact
# Adjust pause durations and speaking rates
```

### Solution Implementation
```yaml
duration_optimization:
  calculation_improvement:
    base_wpm: "Use empirically determined 155 WPM for Amelia voice"
    content_adjustment: "Apply content-type multipliers"
    ssml_accounting: "Include estimated SSML timing impact"

  content_optimization:
    ssml_standardization: "Consistent pause and emphasis patterns"
    content_structuring: "Balance technical and narrative content"
    duration_targets: "Plan content to specific duration goals"

  validation_process:
    duration_checking: "Validate audio duration against targets"
    feedback_loop: "Adjust calculations based on actual results"
    quality_gates: "Duration validation before final production"
```

---

## üéØ QUALITY VALIDATION ISSUES

### STT Validation Accuracy Problems
**Problem**: Speech-to-text validation shows discrepancies with source script
**Impact**: Quality assurance false positives/negatives

#### Diagnostic Framework
```yaml
stt_validation_analysis:
  accuracy_factors:
    voice_quality: "Higher quality voices improve STT accuracy"
    content_complexity: "Technical terms reduce STT accuracy"
    pronunciation: "Mispronunciations create validation failures"
    audio_quality: "Professional audio settings improve STT results"

  validation_metrics:
    word_accuracy: "Target >95% word-level accuracy"
    semantic_accuracy: "Target >98% meaning preservation"
    technical_accuracy: "Target >90% for technical terms"
```

#### Solution Approach
```yaml
stt_optimization:
  audio_quality:
    lufs_target: "-16 LUFS for professional broadcast quality"
    sample_rate: "48kHz for maximum clarity"
    bit_depth: "24-bit for professional standard"

  content_optimization:
    technical_terms: "Phonetic spelling in SSML for complex terms"
    pronunciation_guide: "Custom pronunciation dictionaries"
    content_review: "Technical accuracy validation before synthesis"

  validation_framework:
    multi_pass_checking: "Multiple validation approaches for accuracy"
    semantic_comparison: "Meaning-based validation vs literal matching"
    quality_thresholds: "Adjustable accuracy thresholds by content type"
```

---

## üí∞ COST & BILLING ISSUES

### Unexpected Cost Overruns
**Problem**: Episode production costs exceed budget estimates
**Impact**: Project budget management and scalability concerns

#### Cost Tracking Framework
```yaml
cost_monitoring:
  real_time_tracking:
    pre_tool_hooks: "Budget validation before API calls"
    post_tool_hooks: "Cost accumulation and attribution"
    phase_budgets: "Per-phase cost limits and alerts"

  cost_attribution:
    research_costs: "Perplexity, web search, content gathering"
    script_costs: "LLM usage for content generation"
    audio_costs: "ElevenLabs TTS synthesis"
    validation_costs: "Quality assurance and review processes"

  budget_controls:
    episode_limit: "$15.00 maximum per episode"
    target_cost: "$5.51 optimized production cost"
    alert_thresholds: "75% warning, 90% critical, 100% blocking"
```

#### Cost Optimization Solutions
```yaml
cost_reduction_strategies:
  research_efficiency:
    caching: "Store and reuse research results"
    strategic_queries: "Optimize Perplexity usage patterns"
    free_resources: "Prioritize included/free research tools"

  script_optimization:
    templates: "Standardized structures reduce iteration"
    quality_gates: "Prevent expensive rewrites"
    efficient_prompts: "Reduce token usage through optimization"

  audio_efficiency:
    voice_consistency: "Single production voice reduces costs"
    ssml_optimization: "Efficient markup reduces character count"
    batch_processing: "Optimize API usage patterns"
```

---

## üñ•Ô∏è SYSTEM & INFRASTRUCTURE ISSUES

### Environment Setup Problems
**Problem**: Installation, configuration, and environment issues
**Impact**: Unable to start or use the podcast production system

#### Common Setup Issues
```yaml
environment_problems:
  python_version:
    issue: "Incompatible Python version"
    solution: "Use Python 3.11+ with virtual environment"
    verification: "python3 --version && which python3"

  virtual_environment:
    issue: "Virtual environment not activated or configured"
    solution: "Create and activate proper virtual environment"
    verification: "which python && pip list"

  package_installation:
    issue: "Missing or incompatible packages"
    solution: "Install requirements in clean virtual environment"
    verification: "pip install -r requirements.txt"

  api_configuration:
    issue: "API keys not properly configured"
    solution: "Create .env file with all required keys"
    verification: "source .env && env | grep -E '(ELEVENLABS|PERPLEXITY|GITHUB)'"
```

#### System Diagnostic Checklist
```bash
#!/bin/bash
# System readiness diagnostic

echo "=== AI Podcast System Diagnostics ==="

# Check Python environment
echo "1. Python Environment:"
python3 --version
which python3

# Check virtual environment
echo "2. Virtual Environment:"
if [[ "$VIRTUAL_ENV" ]]; then
    echo "‚úÖ Virtual environment active: $VIRTUAL_ENV"
else
    echo "‚ùå No virtual environment detected"
fi

# Check API keys
echo "3. API Key Configuration:"
if [[ -f .env ]]; then
    echo "‚úÖ .env file exists"
    source .env
    echo "ELEVENLABS_API_KEY: $([ ! -z "$ELEVENLABS_API_KEY" ] && echo 'SET' || echo 'NOT SET')"
    echo "PERPLEXITY_API_KEY: $([ ! -z "$PERPLEXITY_API_KEY" ] && echo 'SET' || echo 'NOT SET')"
    echo "GITHUB_PAT: $([ ! -z "$GITHUB_PAT" ] && echo 'SET' || echo 'NOT SET')"
else
    echo "‚ùå .env file missing"
fi

# Check Claude Code installation
echo "4. Claude Code:"
if command -v claude &> /dev/null; then
    echo "‚úÖ Claude Code installed"
else
    echo "‚ùå Claude Code not found"
fi

# Check directory structure
echo "5. Project Structure:"
for dir in .claude/agents .claude/commands .claude/context; do
    if [[ -d "$dir" ]]; then
        echo "‚úÖ $dir exists"
    else
        echo "‚ùå $dir missing"
    fi
done
```

---

## üîç DIAGNOSTIC WORKFLOWS

### Quick MCP Diagnostics
```bash
# Step 1: Check MCP server status
/mcp

# Expected output:
# ‚éø MCP Server Status ‚éø
# ‚éø ‚Ä¢ perplexity-ask: connected ‚éø
# ‚éø ‚Ä¢ ElevenLabs: connected ‚éø

# Step 2: Test MCP tools directly
mcp__elevenlabs__check_subscription
mcp__perplexity-ask__perplexity_ask

# Step 3: Verify agent MCP inheritance
# Use Task tool with agent requiring MCP access
```

### Agent Configuration Validation
```bash
# Check agent YAML syntax
python3 -c "
import yaml
import sys
try:
    with open('.claude/agents/audio-synthesizer.md', 'r') as f:
        content = f.read()
    # Extract YAML from markdown
    yaml_start = content.find('```yaml')
    yaml_end = content.find('```', yaml_start + 7)
    yaml_content = content[yaml_start+7:yaml_end]
    yaml.safe_load(yaml_content)
    print('‚úÖ Valid YAML configuration')
except Exception as e:
    print(f'‚ùå YAML Error: {e}')
"
```

### Performance Monitoring
```yaml
performance_diagnostics:
  system_metrics:
    memory_usage: "Monitor RAM consumption during processing"
    cpu_utilization: "Track processing load during synthesis"
    disk_space: "Ensure adequate storage for audio files"
    network_latency: "Monitor API response times"

  application_metrics:
    api_response_times: "Track ElevenLabs and Perplexity response speeds"
    error_rates: "Monitor failure rates across different operations"
    cost_per_operation: "Track spending efficiency"
    quality_scores: "Monitor output quality consistency"
```

---

## üõ†Ô∏è EMERGENCY PROCEDURES

### System Recovery Protocol
```yaml
emergency_recovery:
  mcp_failure_recovery:
    step_1: "Restart Claude Code with proper environment loading"
    step_2: "Verify all API keys accessible to MCP servers"
    step_3: "Test individual MCP tools before agent usage"
    step_4: "Clear any cached configurations that might be corrupted"

  production_halt_recovery:
    step_1: "Identify root cause using diagnostic scripts"
    step_2: "Implement temporary workaround if available"
    step_3: "Execute systematic fix based on issue classification"
    step_4: "Validate fix with comprehensive testing"

  cost_overrun_emergency:
    step_1: "Immediately halt all API operations"
    step_2: "Analyze cost attribution to identify source"
    step_3: "Implement cost controls and budget limits"
    step_4: "Resume operations with enhanced monitoring"
```

This unified troubleshooting guide consolidates all troubleshooting knowledge while providing systematic diagnostic and resolution procedures for all common issues in the AI podcast production system.
