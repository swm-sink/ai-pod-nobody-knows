<?xml version="1.0" encoding="UTF-8"?>
<document type="learning-guide" domain="claude-code" version="3.0" xmlns="https://ai-podcasts-nobody-knows.com/claude-docs">
    <metadata>
        <title>Subagents Guide - Master AI Task Delegation and Multi-Agent Orchestration</title>
        <phase>run</phase>
        <skill-level>advanced</skill-level>
        <claude-optimization>true</claude-optimization>
        <learning-outcomes>
            <outcome>Master Claude Code's Task tool for complex multi-step workflows</outcome>
            <outcome>Build sophisticated AI agent orchestration patterns</outcome>
            <outcome>Create specialized subagent workflows for podcast production</outcome>
            <outcome>Implement parallel processing and batch operations with subagents</outcome>
        </learning-outcomes>
        <prerequisites>
            <prerequisite>Files 19-21 (thinking modes, hooks automation, MCP integration)</prerequisite>
        </prerequisites>
        <estimated-time>6-8 hours</estimated-time>
    </metadata>

    <content>
        <section type="introduction" id="overview">
            <technical-explanation>
                Claude Code's Task tool enables sophisticated AI agent delegation where specialized subagents can be assigned complex, multi-step tasks with specific contexts, instructions, and success criteria, allowing for parallel processing, domain-specific expertise application, and scalable automation workflows.

                Subagents transform Claude Code from a single-AI interaction into a multi-agent orchestration platform where complex workflows are broken down into specialized tasks handled by expert AI assistants, enabling sophisticated automation while maintaining human oversight and control.
            </technical-explanation>
            <simple-explanation>
                Think of subagents like having a team of specialized assistants who are experts in different areas - one might be great at research, another at writing, another at analysis. Instead of you doing everything yourself, you can delegate specific tasks to the right expert, give them clear instructions, and they'll work independently while reporting back to you.

                It's like being a project manager who can clone their best team members for different jobs, each with specialized skills and knowledge.
            </simple-explanation>
        </section>

        <section type="implementation" id="task-tool-basics">
            <instructions>
                <step number="1" validation-command="claude task --help">
                    Learn the Task tool syntax and options
                </step>
                <step number="2" validation-command="echo 'Analyze this file structure and suggest improvements' | claude task --context .claude/context/">
                    Create a basic subagent task with context
                </step>
                <step number="3" validation-command="mkdir -p .claude/subagent-logs">
                    Create directory for tracking subagent results
                </step>
            </instructions>

            <examples>
                <example type="basic">
                    <scenario>Code analysis subagent for quality improvement</scenario>
                    <implementation>
# Delegate code analysis to specialized subagent
claude task "Analyze the podcast production code in .claude/level-2-production/ and provide:
1. Code quality assessment
2. Architecture improvement suggestions
3. Performance optimization opportunities
4. Security considerations
5. Maintainability recommendations

Focus on AI orchestration patterns and provide specific, actionable advice." \
--context ".claude/level-2-production/" \
--output ".claude/subagent-logs/code-analysis-$(date +%Y%m%d).md"
                    </implementation>
                    <explanation>This creates a specialized code analysis subagent with specific context and clear deliverables, enabling expert-level code review without requiring you to manually analyze every file.</explanation>
                </example>

                <example type="advanced">
                    <scenario>Parallel episode quality analysis across multiple episodes</scenario>
                    <implementation>
# Spawn multiple subagents for parallel processing
for episode in episodes/ep00{1..5}*; do
  claude task "Analyze episode $episode for:
  1. Content quality score (0-1.0)
  2. Brand voice consistency
  3. Duration accuracy vs 27-minute target
  4. Research source diversity
  5. Engagement potential

  Compare against our quality standards and provide improvement recommendations." \
  --context "$episode" \
  --output ".claude/subagent-logs/quality-analysis-$(basename $episode).json" &
done

# Wait for all subagents to complete
wait
echo "✅ All episode analyses complete"
                    </implementation>
                    <explanation>This demonstrates advanced parallel processing where multiple subagents analyze different episodes simultaneously, dramatically reducing the time needed for comprehensive quality assessment across a large content library.</explanation>
                </example>
            </examples>

            <validation>
                <validation-command>ls .claude/subagent-logs/ && claude task --list-active</validation-command>
                <success-criteria>Subagent logs directory exists and task system is operational</success-criteria>
            </validation>
        </section>

        <section type="implementation" id="specialized-subagents">
            <instructions>
                <step number="1" validation-command="touch .claude/subagent-templates/research-analyst.md">
                    Create template for research analysis subagent
                </step>
                <step number="2" validation-command="touch .claude/subagent-templates/content-optimizer.md">
                    Create template for content optimization subagent
                </step>
                <step number="3" validation-command="touch .claude/subagent-templates/quality-validator.md">
                    Create template for quality validation subagent
                </step>
            </instructions>

            <examples>
                <example type="basic">
                    <scenario>Research validation subagent for source credibility</scenario>
                    <implementation>
# Create specialized research validation subagent
claude task "Act as a Research Validation Specialist. Analyze the research file and provide:

1. Source Credibility Assessment:
   - Academic/institutional sources (universities, journals)
   - Primary vs secondary sources
   - Publication recency and relevance
   - Author expertise and credentials

2. Information Quality Analysis:
   - Fact accuracy and verifiability
   - Potential bias or conflicts of interest
   - Comprehensiveness of coverage
   - Missing perspectives or gaps

3. Citation and Attribution Review:
   - Proper citation formatting
   - Source accessibility for verification
   - Attribution accuracy

4. Improvement Recommendations:
   - Additional sources to strengthen research
   - Fact-checking priorities
   - Research methodology enhancements

Format as structured JSON with scores (0-1.0) and specific recommendations." \
--context "episodes/ep001-consciousness/research.json" \
--output ".claude/subagent-logs/research-validation-ep001.json"
                    </implementation>
                    <explanation>This creates a domain expert subagent specifically for research validation, applying specialized knowledge about source credibility, academic standards, and fact-checking methodologies to improve research quality.</explanation>
                </example>
            </examples>
        </section>

        <section type="troubleshooting" id="common-issues">
            <technical-explanation>
                Common subagent challenges include context overload, unclear task specifications, parallel processing conflicts, and result aggregation complexity that can lead to inconsistent or incomplete outputs.
            </technical-explanation>
            <simple-explanation>
                What to do when your AI assistants don't deliver the results you expect - like giving clearer instructions, managing their workload, or ensuring they have the right information to work with.
            </simple-explanation>

            <examples>
                <example type="anti-pattern">
                    <scenario>Vague subagent instructions leading to poor results</scenario>
                    <implementation>
# BAD: Vague, unclear task delegation
claude task "Look at this episode and make it better"

# GOOD: Specific, measurable task with clear success criteria
claude task "Analyze episode script for brand voice consistency. Specifically:

1. Count intellectual humility phrases (target: ≥3 per section)
2. Identify curiosity expressions and wonder statements
3. Check accessibility of complex concepts
4. Measure uncertainty acknowledgment frequency
5. Score overall brand consistency (0-1.0)

Provide specific quotes as examples and actionable improvement suggestions for any scores below 0.85."
                    </implementation>
                    <explanation>Subagents need specific, measurable instructions to deliver useful results. Vague requests lead to generic responses, while detailed specifications with success criteria produce actionable, high-quality outputs.</explanation>
                </example>
            </examples>
        </section>

        <section type="advanced" id="orchestration-patterns">
            <technical-explanation>
                Advanced subagent orchestration involves workflow composition, conditional branching, error handling, and result aggregation patterns that create sophisticated multi-agent systems capable of handling complex, multi-step processes with quality assurance and optimization feedback loops.
            </technical-explanation>
            <simple-explanation>
                Power-user techniques for creating teams of AI assistants that work together on complex projects, handle errors gracefully, and automatically improve their performance over time.
            </simple-explanation>

            <examples>
                <example type="advanced">
                    <scenario>Multi-stage content production pipeline with quality gates</scenario>
                    <implementation>
#!/bin/bash
# Advanced subagent orchestration pipeline
EPISODE_ID="ep001-consciousness"

# Stage 1: Parallel research analysis
claude task "Research Analyst: Analyze topic depth and source diversity" \
  --context "episodes/$EPISODE_ID/research.json" \
  --output ".claude/pipeline/research-analysis.json" &

claude task "Fact Checker: Validate claims and identify verification needs" \
  --context "episodes/$EPISODE_ID/research.json" \
  --output ".claude/pipeline/fact-check.json" &

# Wait for research stage completion
wait

# Stage 2: Content optimization (conditional on research quality)
RESEARCH_SCORE=$(jq '.overall_score' .claude/pipeline/research-analysis.json)
if (( $(echo "$RESEARCH_SCORE >= 0.85" | bc -l) )); then

  claude task "Script Optimizer: Enhance narrative flow and engagement" \
    --context "episodes/$EPISODE_ID/" \
    --include ".claude/pipeline/research-analysis.json" \
    --output ".claude/pipeline/script-optimization.json" &

  claude task "Brand Voice Validator: Ensure consistency with intellectual humility" \
    --context "episodes/$EPISODE_ID/script.md" \
    --output ".claude/pipeline/brand-validation.json" &

  wait

  # Stage 3: Final quality gate
  claude task "Production Readiness Evaluator: Comprehensive pre-production check" \
    --context "episodes/$EPISODE_ID/" \
    --include ".claude/pipeline/*.json" \
    --output ".claude/pipeline/production-readiness.json"

  echo "✅ Episode $EPISODE_ID pipeline complete"
else
  echo "❌ Research quality insufficient ($RESEARCH_SCORE < 0.85) - pipeline halted"
fi
                    </implementation>
                    <explanation>This advanced orchestration demonstrates multi-stage workflows with quality gates, parallel processing, conditional logic, and comprehensive context sharing between subagents, creating a sophisticated content production system with built-in quality assurance.</explanation>
                </example>
            </examples>
        </section>
    </content>

    <cross-references>
        <reference file="19_thinking_modes_guide.xml" section="overview" type="prerequisite">
            Enhanced thinking modes that improve subagent reasoning
        </reference>
        <reference file="21_mcp_integration_guide.xml" section="overview" type="related">
            MCP servers that subagents can leverage for external integrations
        </reference>
        <reference file="23_optimization_guide.xml" section="overview" type="advanced">
            Optimization techniques for subagent performance and cost efficiency
        </reference>
    </cross-references>
</document>
